{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior Predictive Modeling\n",
    "\n",
    "This guide provides an introduction to prior predictive modeling using PyMC (and PyMC Marketing) and the {class}`Prior <pymc_marketing.prior.Prior>` class from PyMC-Marketing. Before diving into the technical details, let's understand why priors are crucial in Bayesian analysis and their practical importance in industry applications.\n",
    "\n",
    "## Understanding Bayesian Inference\n",
    "\n",
    "Bayesian inference is based on Bayes' theorem, which provides a formal way to update our beliefs about parameters $\\theta$ (say, saturation or decay rate in a marketing mix models) given observed data $y$:\n",
    "\n",
    "$$p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)}$$\n",
    "\n",
    "Where:\n",
    "- $p(\\theta|y)$ is the posterior probability (what we want to learn)\n",
    "- $p(y|\\theta)$ is the likelihood (how the data is generated)\n",
    "- $p(\\theta)$ is the prior probability (our initial beliefs)\n",
    "- $p(y)$ is the evidence (a normalizing constant), which can be written as $p(y) = \\displaystyle{ \\int p(y|\\theta)p(\\theta)d\\theta }$\n",
    "\n",
    "The posterior distribution combines our prior knowledge with the observed data to give us updated beliefs about the parameters. In practice, we often work with the unnormalized posterior:\n",
    "\n",
    "$$p(\\theta|y) \\propto p(y|\\theta)p(\\theta)$$\n",
    "\n",
    "This is because the normalizing constant $p(y)$ is often intractable to compute directly.\n",
    "\n",
    "### Why Priors Matter in Industry\n",
    "\n",
    "In industry applications, priors serve several crucial purposes:\n",
    "\n",
    "1. **Domain Knowledge Integration**:\n",
    "   - Incorporating expert knowledge into models\n",
    "   - Leveraging historical data from similar projects\n",
    "   - Encoding business constraints and requirements\n",
    "\n",
    "2. **Risk Management**:\n",
    "   - Preventing unrealistic predictions\n",
    "   - Ensuring stable model behavior\n",
    "   - Managing uncertainty in decision-making\n",
    "\n",
    "3. **Data Efficiency**:\n",
    "   - Making models work with limited data\n",
    "   - Faster convergence to reasonable solutions\n",
    "   - Robust predictions in new scenarios\n",
    "\n",
    "4. **Model Regularization**:\n",
    "   - Preventing overfitting\n",
    "   - Handling multicollinearity\n",
    "   - Dealing with sparse data\n",
    "\n",
    "### Common Prior Specification Scenarios\n",
    "\n",
    "In marketing analytics, you'll often encounter these scenarios:\n",
    "\n",
    "1. **Marketing Mix Models**:\n",
    "   - Media channel effectiveness (typically positive)\n",
    "   - Diminishing returns (shape constraints)\n",
    "   - Lift tests calibration\n",
    "\n",
    "2. **Customer Lifetime Value**:\n",
    "   - Purchase rates (positive values)\n",
    "   - Churn probabilities (between 0 and 1)\n",
    "   - Monetary value distributions (positive, often log-normal)\n",
    "\n",
    "3. **A/B Testing**:\n",
    "   - Conversion rates (bounded between 0 and 1)\n",
    "   - Lift measurements (centered around small effects)\n",
    "   - Revenue impacts (potentially heavy-tailed)\n",
    "\n",
    "## What is Prior Predictive Modeling?\n",
    "\n",
    "Prior predictive modeling is a crucial step in Bayesian workflow that helps us validate our prior choices before seeing the actual data. The process involves:\n",
    "\n",
    "1. **Specification**: \n",
    "   - Define prior distributions for model parameters\n",
    "   - Encode domain knowledge and constraints\n",
    "   - Document assumptions and choices\n",
    "\n",
    "2. **Simulation**:\n",
    "   - Sample parameters from prior distributions\n",
    "   - Generate synthetic data using the model structure\n",
    "   - Create multiple scenarios of possible outcomes\n",
    "\n",
    "3. **Validation**:\n",
    "   - Check if simulated data matches domain expertise\n",
    "   - Verify that impossible scenarios are excluded\n",
    "   - Ensure reasonable coverage of possible outcomes\n",
    "\n",
    "### Benefits in Practice\n",
    "\n",
    "1. **Early Problem Detection**:\n",
    "   - Identify unrealistic assumptions\n",
    "   - Catch numerical issues before model fitting\n",
    "   - Validate model structure\n",
    "\n",
    "2. **Stakeholder Communication**:\n",
    "   - Visualize model implications\n",
    "   - Justify modeling choices\n",
    "   - Set realistic expectations\n",
    "\n",
    "3. **Model Development**:\n",
    "   - Iterate on prior choices efficiently\n",
    "   - Compare alternative specifications\n",
    "   - Document model evolution\n",
    "\n",
    "4. **Risk Assessment**:\n",
    "   - Understand model limitations\n",
    "   - Identify edge cases\n",
    "   - Plan for failure modes\n",
    "\n",
    "The prior predictive distribution $p(y)$ represents our beliefs about the data before we observe it. Mathematically, it's the distribution of the data marginalized over the prior:\n",
    "\n",
    "$$p(y) = \\int p(y|\\theta)p(\\theta)d\\theta$$\n",
    "\n",
    "In practice, we can sample from this distribution by:\n",
    "1. Drawing parameters from the prior: $\\theta^{(s)} \\sim p(\\theta)$\n",
    "2. Generating data from the likelihood: $y^{(s)} \\sim p(y|\\theta^{(s)})$\n",
    "\n",
    "This process helps us validate our model in several ways:\n",
    "\n",
    "1. **Parameter Space Coverage**: \n",
    "   The samples $\\{\\theta^{(s)}\\}_{s=1}^S$ show us what parameter values we consider plausible\n",
    "\n",
    "2. **Data Space Coverage**: \n",
    "   The samples $\\{y^{(s)}\\}_{s=1}^S$ show us what data our model can generate\n",
    "\n",
    "3. **Model Sensitivity**: \n",
    "   The relationship between $\\theta^{(s)}$ and $y^{(s)}$ shows how parameters influence predictions\n",
    "\n",
    "Let's explore these concepts through practical examples using the {class}`Prior <pymc_marketing.prior.Prior>` class from PyMC-Marketing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import preliz as pz\n",
    "import pymc as pm\n",
    "import seaborn as sns\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "from pymc_marketing.mmm import MMM, GeometricAdstock, LogisticSaturation\n",
    "from pymc_marketing.paths import data_dir\n",
    "from pymc_marketing.prior import Prior\n",
    "\n",
    "seed: int = sum(map(ord, \"prior\"))\n",
    "rng: np.random.Generator = np.random.default_rng(seed=seed)\n",
    "\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 6]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example: Normal Distribution\n",
    "\n",
    "Let's start with a simple example using a normal distribution. We'll:\n",
    "1. Generate a synthetic dataset\n",
    "2. Study the observed distribution\n",
    "3. Set a prior for the mean and standard deviation\n",
    "4. Sample from its prior predictive distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate a synthetic dataset from a normal distribution with mean one and standard deviation two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mu: float = 1.0\n",
    "true_sigma: float = 2.0\n",
    "n_observations: int = 200\n",
    "\n",
    "data: NDArray = rng.normal(loc=true_mu, scale=true_sigma, size=n_observations)\n",
    "\n",
    "sample_mean: float = data.mean()\n",
    "sample_std: float = data.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the observed data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.kdeplot(data, fill=True, color=\"C0\", ax=ax)\n",
    "ax.axvline(\n",
    "    sample_mean, color=\"C1\", linestyle=\"--\", label=f\"Sample Mean: {sample_mean:.2f}\"\n",
    ")\n",
    "ax.axvline(true_mu, color=\"C2\", linestyle=\"--\", label=f\"True Mean: {true_mu:.2f}\")\n",
    "ax.legend()\n",
    "ax.set(title=\"Observed Data\", xlabel=\"Value\", ylabel=\"Density\")\n",
    "ax.set_title(\"Observed Data\", fontsize=16, fontweight=\"bold\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume we do not know the true mean and standard deviation of the data (as in almost all cases). Our idea is to fit a bayesian model to try to recover the true parameters.\n",
    "\n",
    "We consider the parametric form of the data:\n",
    "\n",
    "$$\n",
    "y \\sim \\text{Normal}(\\mu, \\sigma)\n",
    "$$\n",
    "\n",
    "What could be sensible priors for the mean and standard deviation? This is where the prior predictive modeling comes in. \n",
    "\n",
    "### Fixed Mean and Standard Deviation\n",
    "\n",
    "First, we consider the simple case where we set fixed values for the mean and standard deviation and we sample from the prior predictive distribution (the normal distribution). \n",
    "\n",
    "Consider the following values for the mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_1, mu_2 = -2, 0\n",
    "sigma_1, sigma_2 = 0.5, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these values, we can simply sample from a normal distribution with these parameters and compare it with the observed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=4,\n",
    "    ncols=1,\n",
    "    figsize=(10, 10),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (mu, sigma) in enumerate(product((mu_1, mu_2), (sigma_1, sigma_2))):\n",
    "    ax = axes[i]\n",
    "    normal_prior = pz.Normal(mu=mu, sigma=sigma)\n",
    "    normal_prior.plot_pdf(color=\"C1\", ax=ax)\n",
    "    sns.kdeplot(data, fill=True, color=\"C0\", label=\"Observed Data\", ax=ax)\n",
    "    ax.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    ax.set(xlabel=\"Value\", ylabel=\"Density\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Prior Predictive Samples (fixed mean and std)\",\n",
    "    fontsize=16,\n",
    "    fontweight=\"bold\",\n",
    "    y=0.93,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plots above, we can see how different combinations of mean (μ) and standard deviation (σ) parameters affect the fit to our observed data:\n",
    "\n",
    "- $\\mu=-2, \\sigma=0.5$: The distribution is too narrow and centered too far to the left\n",
    "- $\\mu=-2, \\sigma=3$: Better spread but still centered too far left\n",
    "- $\\mu=0, \\sigma=0.5$: Better centered but too narrow\n",
    "- $\\mu=0, \\sigma=3$: This appears to be the best fit, with both good center and spread matching the observed data\n",
    "\n",
    "This visual comparison helps us understand that priors centered around $\\mu=0$ with a wider standard deviation ($\\sigma=3$) may be more appropriate for our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Predictive Sampling\n",
    "\n",
    "Now we can take a step forward and instead of fixing the mean and standard deviation, we can set a prior for them. To start, we set the following priors for the mean and standard deviation:\n",
    "\n",
    "- $\\mu \\sim \\text{Normal}(0, 2)$\n",
    "- $\\sigma \\sim \\text{Exponential}(1/3)$\n",
    "\n",
    "We first sample from the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of prior samples\n",
    "n_prior_samples: int = 1_000\n",
    "\n",
    "# Sample from the prior distributions\n",
    "mus = rng.normal(loc=0, scale=2, size=n_prior_samples)\n",
    "sigmas = rng.exponential(scale=3, size=n_prior_samples)\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=2,\n",
    "    ncols=1,\n",
    "    figsize=(10, 10),\n",
    "    sharex=False,\n",
    "    sharey=False,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "sns.kdeplot(mus, fill=True, color=\"C2\", ax=ax[0])\n",
    "ax[0].set(xlabel=\"Value\", ylabel=\"Density\")\n",
    "ax[0].set_title(\n",
    "    \"Prior Predictive Samples (normal mean)\", fontsize=16, fontweight=\"bold\"\n",
    ")\n",
    "\n",
    "sns.kdeplot(sigmas, fill=True, color=\"C3\", ax=ax[1])\n",
    "ax[1].set(xlabel=\"Value\", ylabel=\"Density\")\n",
    "ax[1].set_title(\n",
    "    \"Prior Predictive Samples (exponential std)\", fontsize=16, fontweight=\"bold\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pass these samples through a normal distribution to get the prior predictive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_predictive_samples = rng.normal(loc=mus, scale=sigmas)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.kdeplot(prior_predictive_samples, fill=True, color=\"C1\", ax=ax)\n",
    "sns.kdeplot(data, fill=True, color=\"C0\", ax=ax)\n",
    "ax.set(xlim=(-30, 30))\n",
    "ax.set(xlabel=\"Value\", ylabel=\"Density\")\n",
    "ax.set_title(\"Prior Predictive Samples\", fontsize=16, fontweight=\"bold\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior predictive check shows a good agreement between our simulated data and the prior predictive samples. The overlapping densities indicate that our chosen prior distributions for the mean (normal) and standard deviation (lognormal) are reasonable and can generate data similar to what we observe. This suggests our prior specifications are appropriate for this modeling task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Predictive Sampling with PyMC\n",
    "\n",
    "We start by defining the model in PyMC. Note that we do not need to pass the observed data (yet) in order to sample from the prior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={\"n_obs\": range(data.shape[0])}) as model:\n",
    "    # Define the prior distributions\n",
    "    mu = pm.Normal(\"mu\", mu=0, sigma=2)\n",
    "    sigma = pm.Exponential(\"sigma\", lam=1 / 3)\n",
    "\n",
    "    # Define the likelihood\n",
    "    y = pm.Normal(\"y\", mu=mu, sigma=sigma, dims=\"n_obs\")\n",
    "\n",
    "model.to_graphviz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC offers a convenient way to sample from the prior predictive distribution using the {func}`pymc.sample_prior_predictive <pymc.sample_prior_predictive>` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    # Sample from the prior predictive distribution\n",
    "    idata = pm.sample_prior_predictive(samples=n_prior_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.kdeplot(\n",
    "    idata[\"prior\"][\"y\"].to_numpy().flatten(),\n",
    "    fill=True,\n",
    "    color=\"C1\",\n",
    "    label=\"Prior Predictive Samples\",\n",
    "    ax=ax,\n",
    ")\n",
    "sns.kdeplot(data, fill=True, color=\"C0\", label=\"Observed Data\", ax=ax)\n",
    "ax.legend()\n",
    "ax.set(xlabel=\"Value\", ylabel=\"Density\", xlim=(-30, 30))\n",
    "ax.set_title(\"Prior Predictive Samples\", fontsize=16, fontweight=\"bold\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior predictive samples are very similar to the ones obtained from the manual sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Sampling\n",
    "\n",
    "Now we can fit the model to the observed data and sample from the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition the model on the observed data\n",
    "conditioned_model = pm.observe(model, {\"y\": data})\n",
    "\n",
    "# Sample\n",
    "with conditioned_model:\n",
    "    # Sample from the posterior distribution\n",
    "    idata.extend(pm.sample())\n",
    "    # Sample from the posterior predictive distribution\n",
    "    idata.extend(pm.sample_posterior_predictive(idata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the posterior distributions of the mean and standard deviation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_posterior(idata, var_names=[\"mu\", \"sigma\"], figsize=(12, 5))\n",
    "\n",
    "axes[0].axvline(true_mu, color=\"C2\", linestyle=\"--\", label=f\"True Mean: {true_mu:.1f}\")\n",
    "axes[1].axvline(\n",
    "    true_sigma, color=\"C2\", linestyle=\"--\", label=f\"True Std: {true_sigma:.1f}\"\n",
    ")\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(\"Posterior Distributions\", fontsize=16, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain values very close to the true values (this is known as parameter recovery)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the prior and posterior distributions of the mean and standard deviation parameters. These plots are very handy to understand the impact of the data on the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_dist_comparison(idata, var_names=[\"mu\"], figsize=(12, 7))\n",
    "axes = axes.flatten()\n",
    "for ax in axes:\n",
    "    ax.axvline(true_mu, color=\"C2\", linestyle=\"--\", label=f\"True Mean: {true_mu:.1f}\")\n",
    "    ax.legend()\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(\n",
    "    \"Prior vs Posterior Distributions for the Mean\", fontsize=16, fontweight=\"bold\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_dist_comparison(idata, var_names=[\"sigma\"], figsize=(12, 7))\n",
    "axes = axes.flatten()\n",
    "for ax in axes:\n",
    "    ax.axvline(\n",
    "        true_sigma, color=\"C2\", linestyle=\"--\", label=f\"True Std: {true_sigma:.1f}\"\n",
    "    )\n",
    "    ax.legend()\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(\n",
    "    \"Prior vs Posterior Distributions for the Standard Deviation\",\n",
    "    fontsize=16,\n",
    "    fontweight=\"bold\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the posterior predictive distribution vs the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_ppc(idata, var_names=[\"y\"], num_pp_samples=1_000, figsize=(12, 7))\n",
    "ax.set_title(\"Posterior Predictive Samples\", fontsize=16, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the posterior predictive distribution is very similar to the observed data. This is a good sign that our model is well specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Predictive Sampling for Marketing Mix Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_dir / \"mmm_example.csv\"\n",
    "\n",
    "data_df = pd.read_csv(data_path, parse_dates=[\"date_week\"])\n",
    "\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    nrows=3,\n",
    "    ncols=1,\n",
    "    figsize=(12, 9),\n",
    "    sharex=True,\n",
    "    sharey=False,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "sns.lineplot(data=data_df, x=\"date_week\", y=\"y\", color=\"black\", ax=ax[0])\n",
    "ax[0].set(title=\"Sales\", xlabel=\"Date\", ylabel=\"Sales\")\n",
    "\n",
    "sns.lineplot(data=data_df, x=\"date_week\", y=\"x1\", color=\"C0\", ax=ax[1])\n",
    "ax[1].set(title=r\"$x_1$\", xlabel=\"Date\", ylabel=\"Spend\")\n",
    "\n",
    "sns.lineplot(data=data_df, x=\"date_week\", y=\"x2\", color=\"C1\", ax=ax[2])\n",
    "ax[2].set(title=r\"$x_2$\", xlabel=\"Date\", ylabel=\"Spend\")\n",
    "\n",
    "fig.suptitle(\"Observed MMM Data\", fontsize=16, fontweight=\"bold\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=data_df.assign(y=lambda df: df[\"y\"] / df[\"y\"].max()),\n",
    "    x=\"date_week\",\n",
    "    y=\"y\",\n",
    "    color=\"black\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set(title=\"Sales\", xlabel=\"Date\", ylabel=\"Sales\")\n",
    "ax.set_title(\"Scaled Sales\", fontsize=16, fontweight=\"bold\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_columns = [\"x1\", \"x2\"]\n",
    "\n",
    "n_channels = len(channel_columns)\n",
    "\n",
    "total_spend_per_channel = data_df[channel_columns].sum(axis=0)\n",
    "\n",
    "spend_share = total_spend_per_channel / total_spend_per_channel.sum()\n",
    "\n",
    "prior_sigma = spend_share.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "adstock_alpha_prior = Prior(\"Beta\", alpha=1, beta=3)\n",
    "adstock_alpha_prior.preliz.plot_pdf(ax=ax)\n",
    "ax.set(xlabel=\"Adstock Alpha\", ylabel=\"Density\")\n",
    "ax.set_title(\"Adstock Alpha Prior\", fontsize=16, fontweight=\"bold\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "saturation_lam_prior = Prior(\"Gamma\", alpha=4, beta=2)\n",
    "saturation_lam_prior.preliz.plot_pdf(ax=ax)\n",
    "ax.set(xlabel=\"Saturation Lambda\", ylabel=\"Density\")\n",
    "ax.set_title(\"Saturation Lambda Prior\", fontsize=16, fontweight=\"bold\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    nrows=n_channels,\n",
    "    ncols=1,\n",
    "    figsize=(12, 7),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "for i, sigma in enumerate(prior_sigma):\n",
    "    saturation_beta_prior = Prior(\"HalfNormal\", sigma=sigma)\n",
    "    saturation_beta_prior.preliz.plot_pdf(ax=ax[i])\n",
    "    ax[i].set(xlabel=\"Saturation Beta\", ylabel=\"Density\")\n",
    "    ax[i].set_title(\n",
    "        f\"Saturation Beta Prior for Channel {i+1}\", fontsize=14, fontweight=\"bold\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_config = {\n",
    "    \"intercept\": Prior(\"Normal\", mu=0.5, sigma=0.1),\n",
    "    \"gamma_control\": Prior(\"Normal\", mu=0, sigma=0.01),\n",
    "    \"gamma_fourier\": Prior(\"Laplace\", mu=0, b=0.2),\n",
    "    \"adstock_alpha\": Prior(\"Beta\", alpha=2, beta=3),\n",
    "    \"saturation_beta\": Prior(\"HalfNormal\", sigma=prior_sigma),\n",
    "    \"saturation_lam\": Prior(\"Gamma\", alpha=4, beta=2, dims=\"channel\"),\n",
    "    \"likelihood\": Prior(\"Normal\", sigma=Prior(\"Exponential\", lam=1 / 0.5)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmm = MMM(\n",
    "    model_config=my_model_config,\n",
    "    date_column=\"date_week\",\n",
    "    adstock=GeometricAdstock(l_max=8),\n",
    "    saturation=LogisticSaturation(),\n",
    "    channel_columns=channel_columns,\n",
    "    control_columns=[\"event_1\", \"event_2\", \"t\"],\n",
    "    yearly_seasonality=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_df.drop(\"y\", axis=1)\n",
    "y = data_df[\"y\"]\n",
    "\n",
    "\n",
    "# Generate prior predictive samples\n",
    "_ = mmm.sample_prior_predictive(X, y, samples=2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "mmm.plot_prior_predictive(ax=ax, original_scale=True)\n",
    "ax.legend(loc=\"lower center\", bbox_to_anchor=(0.5, -0.2), ncol=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_channel_contribution_original_scale = (\n",
    "    mmm.compute_channel_contribution_original_scale(prior=True)\n",
    ")\n",
    "\n",
    "spend_sum = X[[\"x1\", \"x2\"]].sum().to_numpy()\n",
    "\n",
    "prior_roas_samples = (\n",
    "    prior_channel_contribution_original_scale.sum(dim=\"date\")\n",
    "    / spend_sum[np.newaxis, np.newaxis, :]\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=2, ncols=1, figsize=(12, 7), sharex=True, sharey=False, layout=\"constrained\"\n",
    ")\n",
    "az.plot_posterior(prior_roas_samples, ax=axes)\n",
    "axes[0].set(title=\"Channel $x_{1}$\")\n",
    "axes[1].set(title=\"Channel $x_{2}$\", xlabel=\"ROAS\")\n",
    "fig.suptitle(\"ROAS Prior Distributions\", fontsize=18, fontweight=\"bold\", y=1.06);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough samples to build a trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.85\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m mmm\u001b[38;5;241m.\u001b[39msample_posterior_predictive(X, extend_idata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, combined\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/pymc-marketing/pymc_marketing/model_builder.py:661\u001b[0m, in \u001b[0;36mModelBuilder.fit\u001b[0;34m(self, X, y, progressbar, predictor_names, random_seed, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m sampler_kwargs \u001b[38;5;241m=\u001b[39m create_sample_kwargs(\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler_config,\n\u001b[1;32m    656\u001b[0m     progressbar,\n\u001b[1;32m    657\u001b[0m     random_seed,\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    659\u001b[0m )\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel:\n\u001b[0;32m--> 661\u001b[0m     idata \u001b[38;5;241m=\u001b[39m \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msampler_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midata:\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midata\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Documents/envs/pymc-marketing-env/lib/python3.12/site-packages/pymc/sampling/mcmc.py:870\u001b[0m, in \u001b[0;36msample\u001b[0;34m(draws, tune, chains, cores, random_seed, progressbar, progressbar_theme, step, var_names, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, blas_cores, model, **kwargs)\u001b[0m\n\u001b[1;32m    866\u001b[0m t_sampling \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_start\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m# Packaging, validating and returning the result was extracted\u001b[39;00m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;66;03m# into a function to make it easier to test and refactor.\u001b[39;00m\n\u001b[0;32m--> 870\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_sampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midata_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/envs/pymc-marketing-env/lib/python3.12/site-packages/pymc/sampling/mcmc.py:901\u001b[0m, in \u001b[0;36m_sample_return\u001b[0;34m(run, traces, tune, t_sampling, discard_tuned_samples, compute_convergence_checks, return_inferencedata, keep_warning_stat, idata_kwargs, model)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;66;03m# Pick and slice chains to keep the maximum number of samples\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m discard_tuned_samples:\n\u001b[0;32m--> 901\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m \u001b[43m_choose_chains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    903\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m _choose_chains(traces, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/envs/pymc-marketing-env/lib/python3.12/site-packages/pymc/backends/base.py:593\u001b[0m, in \u001b[0;36m_choose_chains\u001b[0;34m(traces, tune)\u001b[0m\n\u001b[1;32m    591\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trace) \u001b[38;5;241m-\u001b[39m tune) \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces]\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths):\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough samples to build a trace.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    595\u001b[0m idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(lengths)\n\u001b[1;32m    596\u001b[0m l_sort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(lengths)[idxs]\n",
      "\u001b[0;31mValueError\u001b[0m: Not enough samples to build a trace."
     ]
    }
   ],
   "source": [
    "mmm.fit(X=X, y=y, target_accept=0.85, chains=4, random_seed=rng)\n",
    "mmm.sample_posterior_predictive(X, extend_idata=True, combined=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmm.idata[\"sample_stats\"][\"diverging\"].sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmm.plot_posterior_predictive(original_scale=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_contribution_original_scale = mmm.compute_channel_contribution_original_scale()\n",
    "spend_sum = X[[\"x1\", \"x2\"]].sum().to_numpy()\n",
    "\n",
    "roas_samples = (\n",
    "    channel_contribution_original_scale.sum(dim=\"date\")\n",
    "    / spend_sum[np.newaxis, np.newaxis, :]\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=2, ncols=1, figsize=(12, 7), sharex=True, sharey=False, layout=\"constrained\"\n",
    ")\n",
    "az.plot_posterior(roas_samples, ax=axes)\n",
    "axes[0].set(title=\"Channel $x_{1}$\")\n",
    "axes[1].set(title=\"Channel $x_{2}$\", xlabel=\"ROAS\")\n",
    "fig.suptitle(\"ROAS Posterior Distributions\", fontsize=18, fontweight=\"bold\", y=1.06);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
