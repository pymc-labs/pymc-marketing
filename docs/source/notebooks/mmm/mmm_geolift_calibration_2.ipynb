{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMM Calibration with Geo-Level Lift Tests\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to calibrate a multidimensional MMM using lift test results from a geo-level experiment. By incorporating experimental lift measurements, we can achieve better parameter recovery--both reduced bias and increased precision--compared to fitting the MMM alone.\n",
    "\n",
    "We follow the same pattern as the [national-level lift test notebook](mmm_lift_test.ipynb), generating data directly from the model to ensure perfect consistency.\n",
    "\n",
    "### Notebook Overview\n",
    "\n",
    "This is a long technical document, so here's a roadmap of what we'll cover:\n",
    "\n",
    "**Setup (Sections 1-2)**\n",
    "- We simulate a marketing dataset with **8 geographic regions** and **2 media channels**\n",
    "- The two channels are highly correlated (~0.99), making them difficult to separate--a common real-world challenge\n",
    "\n",
    "**The Experiment (Section 3)**\n",
    "- We run a geo-level lift test on **channel 1 only**\n",
    "- **4 treated geos** receive an incremental spend increase; **4 control geos** remain unchanged\n",
    "- This produces **4 lift test measurements**--one per treated geo--each estimating the causal effect of channel 1\n",
    "\n",
    "**Analysis (Sections 4-6)**\n",
    "- We fit the MMM twice: once without calibration (baseline) and once with lift test calibration\n",
    "- We compare parameter recovery, showing that calibration reduces bias and increases precision\n",
    "- Key result: the calibrated model's saturation curves more closely match the true curves\n",
    "\n",
    "### When to Use Geo-Level Lift Tests\n",
    "\n",
    "Geo-level lift tests are appropriate when you can:\n",
    "- **Target specific geographic regions**: Digital campaigns with location targeting, regional TV markets\n",
    "- **Hold out control regions**: Some geos receive the treatment while others serve as controls\n",
    "- **Measure regional outcomes**: Sales, conversions, or other metrics at the geographic level\n",
    "\n",
    "### The Practical Workflow: From Experiment to Calibration\n",
    "\n",
    "In practice, running a geo-level lift test and using it to calibrate an MMM involves several steps:\n",
    "\n",
    "1. **Design and run the experiment**: Increase (or decrease) spend on one channel in a subset of geographic regions (treated geos), while keeping spend unchanged in the remaining regions (control geos). Run the experiment for several weeks.\n",
    "\n",
    "2. **Analyze the experiment with synthetic control**: Use a method like [CausalPy's multi-cell GeoLift](https://causalpy.readthedocs.io/en/latest/notebooks/multi_cell_geolift.html) to estimate the causal effect. Synthetic control constructs a weighted combination of control geos that matches each treated geo's pre-experiment trend, then measures the post-experiment divergence as the treatment effect.\n",
    "\n",
    "3. **Extract lift test measurements**: From the CausalPy analysis, obtain for each treated geo:\n",
    "   - x: the baseline spend level during the test period\n",
    "   - delta_x: the incremental spend change applied\n",
    "   - delta_y: the estimated causal lift in outcomes\n",
    "   - sigma: the uncertainty (standard error) of the lift estimate\n",
    "\n",
    "4. **Calibrate the MMM**: Pass these lift test measurements to the MMM's add_lift_test_measurements() method to constrain the saturation curve parameters during model fitting.\n",
    "\n",
    "**Scope of this notebook**: This notebook focuses on step 4--demonstrating how lift test measurements improve MMM parameter recovery. We simulate the lift test results directly rather than running a full CausalPy analysis, since this is a PyMC-Marketing tutorial. For the experiment analysis step, see the [CausalPy documentation](https://causalpy.readthedocs.io/en/latest/notebooks/multi_cell_geolift.html).\n",
    "\n",
    "### Why Lift Tests Work: Pinning Down the Saturation Curve\n",
    "\n",
    "It's worth understanding *why* lift tests improve parameter estimation--the benefit goes beyond simply adding more variation to your data.\n",
    "\n",
    "**The problem with observational data alone**\n",
    "\n",
    "When fitting an MMM to observational data, you observe spend (X) and outcomes (Y) moving together. But correlation isn't causation:\n",
    "- Maybe sales rise when spend rises because the spend *caused* more sales\n",
    "- Or maybe both rose because of an external factor (seasonality, promotions, economic conditions)\n",
    "- With highly correlated channels, the model can't tell which channel is actually driving sales--many different parameter combinations fit the data equally well\n",
    "\n",
    "**What a lift test provides: causal ground truth**\n",
    "\n",
    "A lift test is an *experiment*. By randomly assigning some geos to receive increased spend while others serve as controls, the difference in outcomes between groups is a **causal effect**, not just a correlation. The lift test tells you:\n",
    "\n",
    "> \"When we increased channel 1 spend from x to x + delta_x, the true causal lift in sales was delta_y\"\n",
    "\n",
    "This is a point on the saturation curve that we *know* to be correct, not just inferred from correlational patterns.\n",
    "\n",
    "**How calibration uses this information**\n",
    "\n",
    "When we add lift test measurements to the MMM, we're adding a constraint that says: the model's saturation curve must pass through (or near) the experimentally measured point. Mathematically:\n",
    "\n",
    "$$\\text{saturation}(x + \\Delta x) - \\text{saturation}(x) \\approx \\Delta y$$\n",
    "\n",
    "This pins down the saturation curve at the operating point where the experiment was run, dramatically reducing the set of feasible (lambda, beta) parameter values.\n",
    "\n",
    "**The key distinction**\n",
    "\n",
    "| Aspect | More Spend Variation | Lift Test Calibration |\n",
    "|--------|----------------------|----------------------|\n",
    "| **Provides** | More data points | Causal ground truth |\n",
    "| **Helps with** | Precision (narrower posteriors) | Accuracy (reduced bias) |\n",
    "| **Addresses** | Uncertainty in parameters | Bias in parameters |\n",
    "\n",
    "In short: more data variation helps you estimate parameters more precisely, but a lift test helps you estimate the *right* parameters.\n",
    "\n",
    "### Key Design Principles\n",
    "\n",
    "1. **Normalized data**: Channel spend is normalized to [0, 1] range for consistent saturation behavior\n",
    "2. **Model-based data generation**: We use `pm.do` and `pm.draw` to generate synthetic data from the model itself, ensuring the data perfectly matches model assumptions\n",
    "3. **Consistent lift tests**: Lift test measurements are calculated using the same saturation function the model uses\n",
    "\n",
    "## Prepare Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import xarray as xr\n",
    "from pymc_extras.prior import Prior\n",
    "\n",
    "from pymc_marketing.mmm import GeometricAdstock, LogisticSaturation\n",
    "from pymc_marketing.mmm.multidimensional import MMM\n",
    "from pymc_marketing.mmm.transformers import logistic_saturation\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "plt.style.use(\"bmh\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 7)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = sum(map(ord, \"Geo lift tests for MMM calibration\"))\n",
    "rng = np.random.default_rng(seed)\n",
    "print(f\"Random seed: {seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data\n",
    "\n",
    "Our data generation follows a two-step process that ensures perfect consistency between the synthetic data and model assumptions:\n",
    "\n",
    "### Step 1: Manually Simulate Channel Spend (X)\n",
    "\n",
    "We manually create the **input data** — the marketing spend decisions:\n",
    "- `channel_1`, `channel_2`: Normalized spend values in [0, 1] range\n",
    "- `date`: Time dimension (weekly data)\n",
    "- `geo`: Geographic dimension\n",
    "\n",
    "This represents **business decisions** that are external to the model. We intentionally create highly correlated channels to simulate the identification problem that lift tests help solve.\n",
    "\n",
    "### Step 2: Generate Outcomes from the Model (y)\n",
    "\n",
    "We generate the **target variable** (sales/conversions) using `pm.do` and `pm.draw`:\n",
    "1. Build the MMM with the spend data X\n",
    "2. Use `pm.do` to fix model parameters to known \"true\" values\n",
    "3. Use `pm.draw` to sample y from the model\n",
    "\n",
    "This approach ensures that y is generated **exactly as the model expects**, including:\n",
    "- Adstock transformations applied correctly\n",
    "- Saturation curves computed consistently\n",
    "- Noise structure matching the model's likelihood\n",
    "- Internal scaling handled properly\n",
    "\n",
    "**Why not generate y manually?** Manual generation (e.g., `y = intercept + beta * saturation(adstock(x)) + noise`) can introduce subtle mismatches with how the model actually computes predictions, leading to poor parameter recovery.\n",
    "\n",
    "---\n",
    "\n",
    "### Create Normalized Channel Spend Data\n",
    "\n",
    "Following the national-level notebook, we normalize spend data to [0, 1] range. This ensures:\n",
    "- Saturation parameters (lam) have intuitive values (e.g., 5-15)\n",
    "- The model's internal scaling doesn't create mismatches\n",
    "- Lift test measurements are consistent with model assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions\n",
    "n_dates = 104  # 2 years of weekly data\n",
    "geos = [f\"geo_{i:02d}\" for i in range(8)]  # 8 geos for tractable computation\n",
    "n_geos = len(geos)\n",
    "channels = [\"channel_1\", \"channel_2\"]\n",
    "n_channels = len(channels)\n",
    "\n",
    "# Create date range\n",
    "dates = pd.date_range(start=\"2022-01-03\", periods=n_dates, freq=\"W-MON\")\n",
    "\n",
    "print(\"Data dimensions:\")\n",
    "print(f\"  Dates: {n_dates} weeks ({dates[0].date()} to {dates[-1].date()})\")\n",
    "print(f\"  Geos: {n_geos}\")\n",
    "print(f\"  Channels: {n_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate normalized spend data (0-1 range)\n",
    "# Channel 1 and 2 are highly correlated (the identification problem)\n",
    "rows = []\n",
    "\n",
    "for geo in geos:\n",
    "    # Generate base spend pattern (shared across channels for correlation)\n",
    "    base_spend = pm.draw(\n",
    "        pm.Uniform.dist(lower=0.2, upper=1.0, size=n_dates), random_seed=rng\n",
    "    )\n",
    "    base_spend = base_spend / base_spend.max()  # Normalize to max=1\n",
    "\n",
    "    # Add geo-specific scaling\n",
    "    geo_scale = 0.7 + 0.3 * (geos.index(geo) / (n_geos - 1))  # 0.7 to 1.0\n",
    "\n",
    "    for i, date in enumerate(dates):\n",
    "        # Channel 1: base pattern with small noise\n",
    "        ch1 = base_spend[i] * geo_scale + rng.normal(0, 0.02)\n",
    "        ch1 = np.clip(ch1, 0.1, 1.0)\n",
    "\n",
    "        # Channel 2: highly correlated with channel 1 (shifted slightly)\n",
    "        ch2 = base_spend[i] * geo_scale * 0.95 + rng.normal(0, 0.02)\n",
    "        ch2 = np.clip(ch2, 0.1, 1.0)\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"date\": date,\n",
    "                \"geo\": geo,\n",
    "                \"channel_1\": ch1,\n",
    "                \"channel_2\": ch2,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Verify normalization\n",
    "print(\"Channel spend ranges (should be ~0-1):\")\n",
    "for ch in channels:\n",
    "    print(f\"  {ch}: [{df[ch].min():.3f}, {df[ch].max():.3f}]\")\n",
    "\n",
    "# Check correlation\n",
    "corr = df[[\"channel_1\", \"channel_2\"]].corr().iloc[0, 1]\n",
    "print(\n",
    "    f\"\\nChannel correlation: {corr:.3f} (high correlation = identification challenge)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spend patterns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Spend over time for sample geos\n",
    "ax = axes[0]\n",
    "sample_geos = [geos[0], geos[4], geos[7]]\n",
    "for geo in sample_geos:\n",
    "    geo_data = df[df[\"geo\"] == geo]\n",
    "    ax.plot(geo_data[\"date\"], geo_data[\"channel_1\"], label=f\"{geo} - ch1\", alpha=0.7)\n",
    "ax.set_title(\"Channel 1 Spend Over Time (Sample Geos)\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Normalized Spend\")\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Channel correlation\n",
    "ax = axes[1]\n",
    "ax.scatter(df[\"channel_1\"], df[\"channel_2\"], alpha=0.3, s=10)\n",
    "ax.set_title(f\"Channel 1 vs Channel 2 (Correlation: {corr:.3f})\")\n",
    "ax.set_xlabel(\"Channel 1 Spend\")\n",
    "ax.set_ylabel(\"Channel 2 Spend\")\n",
    "ax.set_xlim(0, 1.1)\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define True Parameters\n",
    "\n",
    "We define parameters that work well with normalized [0, 1] spend data:\n",
    "- **Saturation lam**: Values around 5-15 (half-saturation at x ≈ ln(3)/lam ≈ 0.1-0.2)\n",
    "- **Saturation beta**: Contribution coefficients ~0.3-0.8\n",
    "- **Adstock alpha**: Carryover effects ~0.3-0.7\n",
    "\n",
    "Channel 1 and 2 have similar parameters, making them hard to separate without lift tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define true parameters for data generation\n",
    "# These will be used with pm.do to fix the model parameters\n",
    "\n",
    "# Lam values appropriate for normalized [0,1] inputs\n",
    "# Higher lam = saturates faster (half-saturation at ~ln(3)/lam)\n",
    "true_lam_c1 = 8.0  # Half-saturation at ~0.14\n",
    "true_lam_c2 = 6.0  # Half-saturation at ~0.18 (similar to c1)\n",
    "\n",
    "# Beta values (contribution coefficients)\n",
    "true_beta_c1 = 0.6\n",
    "true_beta_c2 = 0.5  # Similar to c1 (hard to separate)\n",
    "\n",
    "# Adstock alpha (carryover)\n",
    "true_alpha_c1 = 0.5\n",
    "true_alpha_c2 = 0.5\n",
    "\n",
    "# Intercept per geo (base level)\n",
    "true_intercept = np.array([0.3 + 0.05 * i for i in range(n_geos)])\n",
    "\n",
    "# Create parameter arrays matching model structure\n",
    "# Note: lam and beta have dims (geo, channel) in multidimensional MMM\n",
    "true_lam = np.array([[true_lam_c1, true_lam_c2] for _ in range(n_geos)])\n",
    "true_beta = np.array([[true_beta_c1, true_beta_c2] for _ in range(n_geos)])\n",
    "\n",
    "true_params = {\n",
    "    \"adstock_alpha\": np.array([true_alpha_c1, true_alpha_c2]),\n",
    "    \"saturation_lam\": true_lam,\n",
    "    \"saturation_beta\": true_beta,\n",
    "    \"intercept_contribution\": true_intercept,\n",
    "    \"y_sigma\": np.full(n_geos, 0.15),  # Noise level per geo\n",
    "}\n",
    "\n",
    "print(\"True parameters:\")\n",
    "print(f\"  Adstock alpha: {true_params['adstock_alpha']}\")\n",
    "print(f\"  Saturation lam (channel 1): {true_lam_c1}, (channel 2): {true_lam_c2}\")\n",
    "print(f\"  Saturation beta (channel 1): {true_beta_c1}, (channel 2): {true_beta_c2}\")\n",
    "print(f\"  Intercept range: [{true_intercept.min():.2f}, {true_intercept.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Target Variable Using the Model\n",
    "\n",
    "Following the national-level notebook, we generate the target variable `y` directly from the model using `pm.do` to fix the true parameters. This ensures **perfect consistency** between data generation and model assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize placeholder y\n",
    "df[\"y\"] = np.ones(len(df))\n",
    "\n",
    "# Prepare data for model\n",
    "X = df[[\"date\", \"geo\", \"channel_1\", \"channel_2\"]].copy()\n",
    "y = df[\"y\"]\n",
    "\n",
    "print(f\"Data shapes: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define priors appropriate for normalized [0,1] inputs\n",
    "adstock_priors = {\n",
    "    \"alpha\": Prior(\"Beta\", alpha=2, beta=2, dims=\"channel\"),  # Centered around 0.5\n",
    "}\n",
    "\n",
    "saturation_priors = {\n",
    "    # Lam prior: Gamma with mean ~8, appropriate for normalized inputs\n",
    "    # Half-saturation at ~ln(3)/8 ≈ 0.14 for mean value\n",
    "    \"lam\": Prior(\"Gamma\", alpha=4, beta=0.5, dims=(\"geo\", \"channel\")),\n",
    "    # Beta prior: moderate contribution\n",
    "    \"beta\": Prior(\"HalfNormal\", sigma=1, dims=(\"geo\", \"channel\")),\n",
    "}\n",
    "\n",
    "print(\"Priors defined:\")\n",
    "print(\"  Adstock alpha: Beta(2, 2) - mean=0.5\")\n",
    "print(\"  Saturation lam: Gamma(4, 0.5) - mean=8\")\n",
    "print(\"  Saturation beta: HalfNormal(1) - mean≈0.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a temporary model to generate data\n",
    "mmm_temp = MMM(\n",
    "    date_column=\"date\",\n",
    "    channel_columns=channels,\n",
    "    adstock=GeometricAdstock(priors=adstock_priors, l_max=8),\n",
    "    saturation=LogisticSaturation(priors=saturation_priors),\n",
    "    dims=(\"geo\",),\n",
    ")\n",
    "\n",
    "mmm_temp.build_model(X, y)\n",
    "print(\"Temporary model built for data generation\")\n",
    "print(f\"Model coords: {list(mmm_temp.model.coords.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate y from the model with fixed true parameters\n",
    "# This ensures perfect consistency between data and model assumptions\n",
    "fixed_model = pm.do(mmm_temp.model, true_params)\n",
    "y_drawn = pm.draw(fixed_model[\"y\"], random_seed=rng)\n",
    "\n",
    "# y_drawn has shape (n_dates, n_geos) with dims (\"date\", \"geo\")\n",
    "# Convert to DataFrame format matching our row order\n",
    "y_xr = xr.DataArray(\n",
    "    y_drawn,\n",
    "    dims=[\"date\", \"geo\"],\n",
    "    coords={\"date\": dates, \"geo\": geos},\n",
    ")\n",
    "y_df = y_xr.to_dataframe(name=\"y\").reset_index()\n",
    "\n",
    "# Merge back to our DataFrame\n",
    "df = df.drop(columns=[\"y\"]).merge(y_df, on=[\"date\", \"geo\"])\n",
    "\n",
    "# Clean up temporary model\n",
    "del mmm_temp.model\n",
    "\n",
    "print(\"Target variable generated from model:\")\n",
    "print(f\"  Shape: {y_drawn.shape} (date, geo)\")\n",
    "print(f\"  Mean: {df['y'].mean():.3f}\")\n",
    "print(f\"  Std: {df['y'].std():.3f}\")\n",
    "print(f\"  Range: [{df['y'].min():.3f}, {df['y'].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize generated data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: y over time for sample geos\n",
    "ax = axes[0]\n",
    "for geo in sample_geos:\n",
    "    geo_data = df[df[\"geo\"] == geo]\n",
    "    ax.plot(geo_data[\"date\"], geo_data[\"y\"], label=geo, alpha=0.7)\n",
    "ax.set_title(\"Target Variable (y) Over Time\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Distribution of y by geo\n",
    "ax = axes[1]\n",
    "df.boxplot(column=\"y\", by=\"geo\", ax=ax)\n",
    "ax.set_title(\"Distribution of y by Geo\")\n",
    "ax.set_xlabel(\"Geo\")\n",
    "ax.set_ylabel(\"y\")\n",
    "plt.suptitle(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit MMM Without Calibration\n",
    "\n",
    "First, let's fit a standard MMM without lift test calibration to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df[[\"date\", \"geo\", \"channel_1\", \"channel_2\"]].copy()\n",
    "y = df[\"y\"]\n",
    "\n",
    "# Initialize MMM (same structure as data generation)\n",
    "mmm_uncalibrated = MMM(\n",
    "    date_column=\"date\",\n",
    "    channel_columns=channels,\n",
    "    adstock=GeometricAdstock(priors=adstock_priors, l_max=8),\n",
    "    saturation=LogisticSaturation(priors=saturation_priors),\n",
    "    dims=(\"geo\",),\n",
    ")\n",
    "\n",
    "mmm_uncalibrated.build_model(X, y)\n",
    "print(\"Uncalibrated model built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using nutpie for faster sampling\n",
    "fit_kwargs = {\n",
    "    \"tune\": 1000,\n",
    "    \"draws\": 1000,\n",
    "    \"chains\": 4,\n",
    "    \"random_seed\": rng,\n",
    "    \"nuts_sampler\": \"nutpie\",\n",
    "}\n",
    "\n",
    "idata_uncalibrated = mmm_uncalibrated.fit(X, y, **fit_kwargs)\n",
    "print(\"\\nUncalibrated model fitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter estimates vs true values\n",
    "posterior = idata_uncalibrated.posterior\n",
    "\n",
    "# We'll focus on geo_idx=0 since parameters are the same across geos\n",
    "geo_idx = 0\n",
    "\n",
    "# Parameter names\n",
    "param_names = [\n",
    "    \"lam (Ch1)\",\n",
    "    \"lam (Ch2)\",\n",
    "    \"beta (Ch1)\",\n",
    "    \"beta (Ch2)\",\n",
    "    \"alpha (Ch1)\",\n",
    "    \"alpha (Ch2)\",\n",
    "]\n",
    "\n",
    "# True values\n",
    "true_values = [\n",
    "    true_lam[0, 0],\n",
    "    true_lam[0, 1],\n",
    "    true_beta[0, 0],\n",
    "    true_beta[0, 1],\n",
    "    true_params[\"adstock_alpha\"][0],\n",
    "    true_params[\"adstock_alpha\"][1],\n",
    "]\n",
    "\n",
    "# Extract posterior samples\n",
    "lam_c1 = posterior[\"saturation_lam\"][:, :, geo_idx, 0].values.flatten()\n",
    "lam_c2 = posterior[\"saturation_lam\"][:, :, geo_idx, 1].values.flatten()\n",
    "beta_c1 = posterior[\"saturation_beta\"][:, :, geo_idx, 0].values.flatten()\n",
    "beta_c2 = posterior[\"saturation_beta\"][:, :, geo_idx, 1].values.flatten()\n",
    "alpha_c1 = posterior[\"adstock_alpha\"][:, :, 0].values.flatten()\n",
    "alpha_c2 = posterior[\"adstock_alpha\"][:, :, 1].values.flatten()\n",
    "\n",
    "samples = [lam_c1, lam_c2, beta_c1, beta_c2, alpha_c1, alpha_c2]\n",
    "\n",
    "# Compute means and HDI\n",
    "est_means = [np.mean(s) for s in samples]\n",
    "hdi_low = [np.percentile(s, 3) for s in samples]\n",
    "hdi_high = [np.percentile(s, 97) for s in samples]\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x_pos = np.arange(len(param_names))\n",
    "bar_width = 0.35\n",
    "\n",
    "# True values as bars\n",
    "ax.bar(\n",
    "    x_pos - bar_width / 2,\n",
    "    true_values,\n",
    "    bar_width,\n",
    "    label=\"True\",\n",
    "    color=\"red\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "# Estimated values as bars with error bars for HDI\n",
    "errors = [\n",
    "    [est_means[i] - hdi_low[i] for i in range(len(est_means))],\n",
    "    [hdi_high[i] - est_means[i] for i in range(len(est_means))],\n",
    "]\n",
    "ax.bar(\n",
    "    x_pos + bar_width / 2,\n",
    "    est_means,\n",
    "    bar_width,\n",
    "    label=\"Uncalibrated (94% HDI)\",\n",
    "    color=\"C0\",\n",
    "    alpha=0.7,\n",
    "    yerr=errors,\n",
    "    capsize=5,\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Parameter\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.set_title(\"Uncalibrated Model: Parameter Estimates vs True Values\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(param_names)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lift Test Measurements\n",
    "\n",
    "Now we simulate a geo-level lift test on **channel 1** in selected treatment geos. \n",
    "\n",
    "**Key principle**: The lift test uses the **same saturation function** that the model uses, ensuring consistency. We calculate:\n",
    "- `x`: baseline (normalized) spend during test period\n",
    "- `delta_x`: incremental spend change\n",
    "- `delta_y`: lift = saturation(x + delta_x) - saturation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the saturation function matching the model\n",
    "def saturation_function(x, lam, beta):\n",
    "    \"\"\"Compute saturation contribution (same as model uses).\"\"\"\n",
    "    return (beta * logistic_saturation(x, lam)).eval()\n",
    "\n",
    "\n",
    "# Create partial functions for each channel with true parameters\n",
    "c1_curve_fn = partial(saturation_function, lam=true_lam_c1, beta=true_beta_c1)\n",
    "c2_curve_fn = partial(saturation_function, lam=true_lam_c2, beta=true_beta_c2)\n",
    "\n",
    "# Visualize the true saturation curves\n",
    "xx = np.linspace(0, 1.2, 100)\n",
    "c1_curve = c1_curve_fn(xx)\n",
    "c2_curve = c2_curve_fn(xx)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    xx,\n",
    "    c1_curve,\n",
    "    label=f\"Channel 1 (lam={true_lam_c1}, beta={true_beta_c1})\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.plot(\n",
    "    xx,\n",
    "    c2_curve,\n",
    "    label=f\"Channel 2 (lam={true_lam_c2}, beta={true_beta_c2})\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.xlabel(\"Normalized Spend (x)\")\n",
    "plt.ylabel(\"Contribution\")\n",
    "plt.title(\"True Saturation Curves (what we want to recover)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lift test setup\n",
    "treated_geos = [geos[i] for i in [0, 2, 4, 6]]  # 4 treated geos\n",
    "control_geos = [geos[i] for i in [1, 3, 5, 7]]  # 4 control geos\n",
    "test_channel = \"channel_1\"\n",
    "\n",
    "print(\"Lift Test Setup:\")\n",
    "print(f\"  Test channel: {test_channel}\")\n",
    "print(f\"  Treated geos: {treated_geos}\")\n",
    "print(f\"  Control geos: {control_geos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lift_test(geo: str, x: float, delta_x: float, sigma: float) -> dict:\n",
    "    \"\"\"\n",
    "    Create a lift test measurement using the true saturation curve.\n",
    "\n",
    "    This directly uses the saturation function, ensuring consistency\n",
    "    with what add_lift_test_measurements() expects.\n",
    "    \"\"\"\n",
    "    delta_y = c1_curve_fn(x + delta_x) - c1_curve_fn(x)\n",
    "\n",
    "    return {\n",
    "        \"channel\": test_channel,\n",
    "        \"geo\": geo,\n",
    "        \"x\": x,\n",
    "        \"delta_x\": delta_x,\n",
    "        \"delta_y\": float(delta_y),\n",
    "        \"sigma\": sigma,\n",
    "    }\n",
    "\n",
    "\n",
    "# Create lift tests at different points on the saturation curve\n",
    "lift_test_results = []\n",
    "\n",
    "for geo in treated_geos:\n",
    "    # Get typical spend level for this geo (from data)\n",
    "    geo_data = df[df[\"geo\"] == geo]\n",
    "    x_typical = geo_data[test_channel].mean()\n",
    "\n",
    "    # Create lift test: increase spend by 0.1 (on 0-1 scale)\n",
    "    delta_x = 0.1\n",
    "    sigma = 0.02  # Measurement uncertainty\n",
    "\n",
    "    lift_test = create_lift_test(geo, x_typical, delta_x, sigma)\n",
    "    lift_test_results.append(lift_test)\n",
    "\n",
    "    print(\n",
    "        f\"{geo}: x={x_typical:.3f}, delta_x={delta_x}, delta_y={lift_test['delta_y']:.4f}\"\n",
    "    )\n",
    "\n",
    "df_lift_test = pd.DataFrame(lift_test_results)\n",
    "print(\"\\nLift Test DataFrame:\")\n",
    "df_lift_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize lift tests on the saturation curve\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot true saturation curve\n",
    "ax.plot(xx, c1_curve, \"--\", color=\"C0\", linewidth=2, label=\"True saturation curve\")\n",
    "\n",
    "# Plot lift test triangles\n",
    "for _, row in df_lift_test.iterrows():\n",
    "    x = row[\"x\"]\n",
    "    delta_x = row[\"delta_x\"]\n",
    "    delta_y = row[\"delta_y\"]\n",
    "    y_base = c1_curve_fn(x)\n",
    "\n",
    "    # Draw triangle showing lift\n",
    "    ax.plot([x, x + delta_x], [y_base, y_base], \"k-\", alpha=0.5)\n",
    "    ax.plot([x + delta_x, x + delta_x], [y_base, y_base + delta_y], \"k-\", alpha=0.5)\n",
    "    ax.scatter([x], [y_base], color=\"C0\", s=80, zorder=5)\n",
    "    ax.scatter(\n",
    "        [x + delta_x],\n",
    "        [y_base + delta_y],\n",
    "        color=\"C2\",\n",
    "        s=80,\n",
    "        zorder=5,\n",
    "        marker=\"^\",\n",
    "        label=f\"{row['geo']}\" if _ == 0 else \"\",\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Normalized Spend (x)\")\n",
    "ax.set_ylabel(\"Contribution\")\n",
    "ax.set_title(f\"Lift Tests on {test_channel} Saturation Curve\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit MMM With Lift Test Calibration\n",
    "\n",
    "Now we fit a new MMM and add the lift test measurements to calibrate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize calibrated MMM with same priors\n",
    "mmm_calibrated = MMM(\n",
    "    date_column=\"date\",\n",
    "    channel_columns=channels,\n",
    "    adstock=GeometricAdstock(priors=adstock_priors, l_max=8),\n",
    "    saturation=LogisticSaturation(priors=saturation_priors),\n",
    "    dims=(\"geo\",),\n",
    ")\n",
    "\n",
    "mmm_calibrated.build_model(X, y)\n",
    "print(\"Calibrated model built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lift test measurements\n",
    "mmm_calibrated.add_lift_test_measurements(df_lift_test)\n",
    "print(f\"Added {len(df_lift_test)} lift test measurements\")\n",
    "print(f\"Lift tests cover geos: {df_lift_test['geo'].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the calibrated model\n",
    "idata_calibrated = mmm_calibrated.fit(X, y, **fit_kwargs)\n",
    "print(\"\\nCalibrated model fitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results: Calibrated vs Uncalibrated\n",
    "\n",
    "Let's compare parameter recovery between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posteriors for comparison\n",
    "posterior_uncal = idata_uncalibrated.posterior\n",
    "posterior_cal = idata_calibrated.posterior\n",
    "\n",
    "# Plot posterior comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Focus on a treated geo\n",
    "focus_geo_idx = 0  # geo_00\n",
    "focus_geo = geos[focus_geo_idx]\n",
    "\n",
    "# Saturation lam - Channel 1\n",
    "ax = axes[0, 0]\n",
    "samples_uncal = posterior_uncal[\"saturation_lam\"][\n",
    "    :, :, focus_geo_idx, 0\n",
    "].values.flatten()\n",
    "samples_cal = posterior_cal[\"saturation_lam\"][:, :, focus_geo_idx, 0].values.flatten()\n",
    "ax.hist(samples_uncal, bins=50, alpha=0.5, label=\"Uncalibrated\", color=\"C0\")\n",
    "ax.hist(samples_cal, bins=50, alpha=0.5, label=\"Calibrated\", color=\"C2\")\n",
    "ax.axvline(true_lam_c1, color=\"red\", linestyle=\"--\", linewidth=2, label=\"True value\")\n",
    "ax.set_title(f\"Saturation lam - Channel 1 ({focus_geo})\")\n",
    "ax.set_xlabel(\"lam\")\n",
    "ax.legend()\n",
    "\n",
    "# Saturation beta - Channel 1\n",
    "ax = axes[0, 1]\n",
    "samples_uncal = posterior_uncal[\"saturation_beta\"][\n",
    "    :, :, focus_geo_idx, 0\n",
    "].values.flatten()\n",
    "samples_cal = posterior_cal[\"saturation_beta\"][:, :, focus_geo_idx, 0].values.flatten()\n",
    "ax.hist(samples_uncal, bins=50, alpha=0.5, label=\"Uncalibrated\", color=\"C0\")\n",
    "ax.hist(samples_cal, bins=50, alpha=0.5, label=\"Calibrated\", color=\"C2\")\n",
    "ax.axvline(true_beta_c1, color=\"red\", linestyle=\"--\", linewidth=2, label=\"True value\")\n",
    "ax.set_title(f\"Saturation beta - Channel 1 ({focus_geo})\")\n",
    "ax.set_xlabel(\"beta\")\n",
    "ax.legend()\n",
    "\n",
    "# Saturation lam - Channel 2 (NOT tested, should be similar)\n",
    "ax = axes[1, 0]\n",
    "samples_uncal = posterior_uncal[\"saturation_lam\"][\n",
    "    :, :, focus_geo_idx, 1\n",
    "].values.flatten()\n",
    "samples_cal = posterior_cal[\"saturation_lam\"][:, :, focus_geo_idx, 1].values.flatten()\n",
    "ax.hist(samples_uncal, bins=50, alpha=0.5, label=\"Uncalibrated\", color=\"C0\")\n",
    "ax.hist(samples_cal, bins=50, alpha=0.5, label=\"Calibrated\", color=\"C2\")\n",
    "ax.axvline(true_lam_c2, color=\"red\", linestyle=\"--\", linewidth=2, label=\"True value\")\n",
    "ax.set_title(f\"Saturation lam - Channel 2 ({focus_geo})\")\n",
    "ax.set_xlabel(\"lam\")\n",
    "ax.legend()\n",
    "\n",
    "# Saturation beta - Channel 2\n",
    "ax = axes[1, 1]\n",
    "samples_uncal = posterior_uncal[\"saturation_beta\"][\n",
    "    :, :, focus_geo_idx, 1\n",
    "].values.flatten()\n",
    "samples_cal = posterior_cal[\"saturation_beta\"][:, :, focus_geo_idx, 1].values.flatten()\n",
    "ax.hist(samples_uncal, bins=50, alpha=0.5, label=\"Uncalibrated\", color=\"C0\")\n",
    "ax.hist(samples_cal, bins=50, alpha=0.5, label=\"Calibrated\", color=\"C2\")\n",
    "ax.axvline(true_beta_c2, color=\"red\", linestyle=\"--\", linewidth=2, label=\"True value\")\n",
    "ax.set_title(f\"Saturation beta - Channel 2 ({focus_geo})\")\n",
    "ax.set_xlabel(\"beta\")\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle(\"Posterior Distributions: Uncalibrated vs Calibrated\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize HDI width comparison (narrower = better precision)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "params_to_check = [\n",
    "    (\"saturation_lam\", 0, \"lam (Ch1)\"),\n",
    "    (\"saturation_beta\", 0, \"beta (Ch1)\"),\n",
    "    (\"saturation_lam\", 1, \"lam (Ch2)\"),\n",
    "    (\"saturation_beta\", 1, \"beta (Ch2)\"),\n",
    "]\n",
    "\n",
    "hdi_widths_uncal = []\n",
    "hdi_widths_cal = []\n",
    "improvements = []\n",
    "param_labels = []\n",
    "\n",
    "for param_name, ch_idx, display_name in params_to_check:\n",
    "    # Average across geos\n",
    "    samples_uncal = posterior_uncal[param_name][:, :, :, ch_idx].values.flatten()\n",
    "    samples_cal = posterior_cal[param_name][:, :, :, ch_idx].values.flatten()\n",
    "\n",
    "    hdi_uncal = az.hdi(samples_uncal, hdi_prob=0.94)\n",
    "    hdi_cal = az.hdi(samples_cal, hdi_prob=0.94)\n",
    "\n",
    "    width_uncal = hdi_uncal[1] - hdi_uncal[0]\n",
    "    width_cal = hdi_cal[1] - hdi_cal[0]\n",
    "    improvement = (width_uncal - width_cal) / width_uncal * 100\n",
    "\n",
    "    hdi_widths_uncal.append(width_uncal)\n",
    "    hdi_widths_cal.append(width_cal)\n",
    "    improvements.append(improvement)\n",
    "    param_labels.append(display_name)\n",
    "\n",
    "# Plot HDI widths\n",
    "x_pos = np.arange(len(param_labels))\n",
    "bar_width = 0.35\n",
    "\n",
    "ax = axes[0]\n",
    "ax.bar(\n",
    "    x_pos - bar_width / 2,\n",
    "    hdi_widths_uncal,\n",
    "    bar_width,\n",
    "    label=\"Uncalibrated\",\n",
    "    color=\"C0\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "ax.bar(\n",
    "    x_pos + bar_width / 2,\n",
    "    hdi_widths_cal,\n",
    "    bar_width,\n",
    "    label=\"Calibrated\",\n",
    "    color=\"C2\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "ax.set_xlabel(\"Parameter\")\n",
    "ax.set_ylabel(\"94% HDI Width\")\n",
    "ax.set_title(\"Posterior Uncertainty (Narrower = More Precise)\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(param_labels)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Plot improvement percentage\n",
    "ax = axes[1]\n",
    "colors = [\"C2\" if imp > 0 else \"C3\" for imp in improvements]\n",
    "ax.bar(x_pos, improvements, color=colors, alpha=0.8)\n",
    "ax.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.set_xlabel(\"Parameter\")\n",
    "ax.set_ylabel(\"HDI Width Reduction (%)\")\n",
    "ax.set_title(\"Precision Improvement from Calibration\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(param_labels)\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, imp in enumerate(improvements):\n",
    "    va = \"bottom\" if imp >= 0 else \"top\"\n",
    "    ax.text(i, imp, f\"{imp:.1f}%\", ha=\"center\", va=va, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saturation Curve Recovery\n",
    "\n",
    "A key benefit of lift test calibration is better recovery of the saturation curves. Let's compare the true curves with the inferred curves from both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot true vs inferred saturation curves with HDI bands\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# x values for plotting curves\n",
    "x_plot = np.linspace(0, 1.2, 100)\n",
    "\n",
    "# Get posterior samples for saturation parameters (focus on first geo)\n",
    "geo_idx = 0\n",
    "\n",
    "\n",
    "def compute_curve_hdi(lam_samples, beta_samples, x_vals, hdi_prob=0.94):\n",
    "    \"\"\"Compute HDI bands for saturation curves.\"\"\"\n",
    "    n_samples = len(lam_samples)\n",
    "    curves = np.zeros((n_samples, len(x_vals)))\n",
    "    for i in range(n_samples):\n",
    "        curves[i, :] = (\n",
    "            beta_samples[i]\n",
    "            * (1 - np.exp(-lam_samples[i] * x_vals))\n",
    "            / (1 + np.exp(-lam_samples[i] * x_vals))\n",
    "        )\n",
    "    # Compute mean and HDI\n",
    "    mean_curve = np.mean(curves, axis=0)\n",
    "    lower = np.percentile(curves, (1 - hdi_prob) / 2 * 100, axis=0)\n",
    "    upper = np.percentile(curves, (1 + hdi_prob) / 2 * 100, axis=0)\n",
    "    return mean_curve, lower, upper\n",
    "\n",
    "\n",
    "for col, (ch_idx, ch_name) in enumerate([(0, \"channel_1\"), (1, \"channel_2\")]):\n",
    "    # True parameters\n",
    "    true_lam = true_params[\"saturation_lam\"][geo_idx, ch_idx]\n",
    "    true_beta = true_params[\"saturation_beta\"][geo_idx, ch_idx]\n",
    "    true_curve = (\n",
    "        true_beta * (1 - np.exp(-true_lam * x_plot)) / (1 + np.exp(-true_lam * x_plot))\n",
    "    )\n",
    "\n",
    "    # Uncalibrated model\n",
    "    ax = axes[0, col]\n",
    "    lam_samples = posterior_uncal[\"saturation_lam\"][\n",
    "        :, :, geo_idx, ch_idx\n",
    "    ].values.flatten()\n",
    "    beta_samples = posterior_uncal[\"saturation_beta\"][\n",
    "        :, :, geo_idx, ch_idx\n",
    "    ].values.flatten()\n",
    "\n",
    "    # Compute HDI bands\n",
    "    mean_uncal, lower_uncal, upper_uncal = compute_curve_hdi(\n",
    "        lam_samples, beta_samples, x_plot\n",
    "    )\n",
    "\n",
    "    ax.fill_between(\n",
    "        x_plot, lower_uncal, upper_uncal, alpha=0.3, color=\"C0\", label=\"94% HDI\"\n",
    "    )\n",
    "    ax.plot(x_plot, mean_uncal, color=\"C0\", linewidth=2, label=\"Posterior mean\")\n",
    "    ax.plot(x_plot, true_curve, \"r--\", linewidth=2, label=\"True curve\")\n",
    "    ax.set_title(f\"Uncalibrated - {ch_name}\")\n",
    "    ax.set_xlabel(\"Normalized Spend\")\n",
    "    ax.set_ylabel(\"Contribution\")\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.0)\n",
    "\n",
    "    # Calibrated model\n",
    "    ax = axes[1, col]\n",
    "    lam_samples = posterior_cal[\"saturation_lam\"][\n",
    "        :, :, geo_idx, ch_idx\n",
    "    ].values.flatten()\n",
    "    beta_samples = posterior_cal[\"saturation_beta\"][\n",
    "        :, :, geo_idx, ch_idx\n",
    "    ].values.flatten()\n",
    "\n",
    "    # Compute HDI bands\n",
    "    mean_cal, lower_cal, upper_cal = compute_curve_hdi(\n",
    "        lam_samples, beta_samples, x_plot\n",
    "    )\n",
    "\n",
    "    ax.fill_between(\n",
    "        x_plot, lower_cal, upper_cal, alpha=0.3, color=\"C2\", label=\"94% HDI\"\n",
    "    )\n",
    "    ax.plot(x_plot, mean_cal, color=\"C2\", linewidth=2, label=\"Posterior mean\")\n",
    "    ax.plot(x_plot, true_curve, \"r--\", linewidth=2, label=\"True curve\")\n",
    "    ax.set_title(f\"Calibrated - {ch_name}\")\n",
    "    ax.set_xlabel(\"Normalized Spend\")\n",
    "    ax.set_ylabel(\"Contribution\")\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.0)\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"Saturation Curve Recovery ({geos[geo_idx]})\\nShaded: 94% HDI, Dashed: True curve\",\n",
    "    fontsize=12,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saturation Parameter Comparison\n",
    "\n",
    "To see the effect of calibration more clearly, let's compare the true, uncalibrated, and calibrated estimates for the saturation parameters side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparing true, uncalibrated, and calibrated saturation parameters\n",
    "# Split into two panels: lambda (left) and beta (right) since they have different scales\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Focus on geo_idx = 0 for clarity\n",
    "geo_idx = 0\n",
    "bar_width = 0.25\n",
    "channel_labels = [\"Channel 1\", \"Channel 2\"]\n",
    "x_pos = np.arange(len(channel_labels))\n",
    "\n",
    "\n",
    "def get_mean_and_hdi(samples, hdi_prob=0.94):\n",
    "    \"\"\"Extract mean and HDI bounds from posterior samples.\"\"\"\n",
    "    flat = samples.values.flatten()\n",
    "    mean = np.mean(flat)\n",
    "    hdi = az.hdi(flat, hdi_prob=hdi_prob)\n",
    "    return mean, hdi[0], hdi[1]\n",
    "\n",
    "\n",
    "# --- Lambda parameters (left panel) ---\n",
    "ax = axes[0]\n",
    "\n",
    "# True values\n",
    "true_lam = [\n",
    "    true_params[\"saturation_lam\"][geo_idx, 0],\n",
    "    true_params[\"saturation_lam\"][geo_idx, 1],\n",
    "]\n",
    "\n",
    "# Uncalibrated: mean and HDI\n",
    "uncal_lam_stats = [\n",
    "    get_mean_and_hdi(posterior_uncal[\"saturation_lam\"][:, :, geo_idx, i])\n",
    "    for i in range(2)\n",
    "]\n",
    "uncal_lam_means = [s[0] for s in uncal_lam_stats]\n",
    "uncal_lam_err = [\n",
    "    [s[0] - s[1] for s in uncal_lam_stats],  # lower error\n",
    "    [s[2] - s[0] for s in uncal_lam_stats],  # upper error\n",
    "]\n",
    "\n",
    "# Calibrated: mean and HDI\n",
    "cal_lam_stats = [\n",
    "    get_mean_and_hdi(posterior_cal[\"saturation_lam\"][:, :, geo_idx, i])\n",
    "    for i in range(2)\n",
    "]\n",
    "cal_lam_means = [s[0] for s in cal_lam_stats]\n",
    "cal_lam_err = [\n",
    "    [s[0] - s[1] for s in cal_lam_stats],\n",
    "    [s[2] - s[0] for s in cal_lam_stats],\n",
    "]\n",
    "\n",
    "# Plot bars with error bars\n",
    "ax.bar(x_pos - bar_width, true_lam, bar_width, label=\"True\", color=\"red\", alpha=0.8)\n",
    "ax.bar(\n",
    "    x_pos,\n",
    "    uncal_lam_means,\n",
    "    bar_width,\n",
    "    label=\"Uncalibrated\",\n",
    "    color=\"C0\",\n",
    "    alpha=0.8,\n",
    "    yerr=uncal_lam_err,\n",
    "    capsize=5,\n",
    ")\n",
    "ax.bar(\n",
    "    x_pos + bar_width,\n",
    "    cal_lam_means,\n",
    "    bar_width,\n",
    "    label=\"Calibrated\",\n",
    "    color=\"C2\",\n",
    "    alpha=0.8,\n",
    "    yerr=cal_lam_err,\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_xlabel(\"Channel\")\n",
    "ax.set_ylabel(\"Lambda (λ)\")\n",
    "ax.set_title(f\"Saturation Lambda Estimates ({geos[geo_idx]})\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(channel_labels)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# --- Beta parameters (right panel) ---\n",
    "ax = axes[1]\n",
    "\n",
    "# True values\n",
    "true_beta = [\n",
    "    true_params[\"saturation_beta\"][geo_idx, 0],\n",
    "    true_params[\"saturation_beta\"][geo_idx, 1],\n",
    "]\n",
    "\n",
    "# Uncalibrated: mean and HDI\n",
    "uncal_beta_stats = [\n",
    "    get_mean_and_hdi(posterior_uncal[\"saturation_beta\"][:, :, geo_idx, i])\n",
    "    for i in range(2)\n",
    "]\n",
    "uncal_beta_means = [s[0] for s in uncal_beta_stats]\n",
    "uncal_beta_err = [\n",
    "    [s[0] - s[1] for s in uncal_beta_stats],\n",
    "    [s[2] - s[0] for s in uncal_beta_stats],\n",
    "]\n",
    "\n",
    "# Calibrated: mean and HDI\n",
    "cal_beta_stats = [\n",
    "    get_mean_and_hdi(posterior_cal[\"saturation_beta\"][:, :, geo_idx, i])\n",
    "    for i in range(2)\n",
    "]\n",
    "cal_beta_means = [s[0] for s in cal_beta_stats]\n",
    "cal_beta_err = [\n",
    "    [s[0] - s[1] for s in cal_beta_stats],\n",
    "    [s[2] - s[0] for s in cal_beta_stats],\n",
    "]\n",
    "\n",
    "# Plot bars with error bars\n",
    "ax.bar(x_pos - bar_width, true_beta, bar_width, label=\"True\", color=\"red\", alpha=0.8)\n",
    "ax.bar(\n",
    "    x_pos,\n",
    "    uncal_beta_means,\n",
    "    bar_width,\n",
    "    label=\"Uncalibrated\",\n",
    "    color=\"C0\",\n",
    "    alpha=0.8,\n",
    "    yerr=uncal_beta_err,\n",
    "    capsize=5,\n",
    ")\n",
    "ax.bar(\n",
    "    x_pos + bar_width,\n",
    "    cal_beta_means,\n",
    "    bar_width,\n",
    "    label=\"Calibrated\",\n",
    "    color=\"C2\",\n",
    "    alpha=0.8,\n",
    "    yerr=cal_beta_err,\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_xlabel(\"Channel\")\n",
    "ax.set_ylabel(\"Beta (β)\")\n",
    "ax.set_title(f\"Saturation Beta Estimates ({geos[geo_idx]})\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(channel_labels)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.suptitle(\"Saturation Parameter Recovery: True vs Estimated (94% HDI)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate posterior predictive and compare\n",
    "mmm_uncalibrated.sample_posterior_predictive(X, extend_idata=True, random_seed=rng)\n",
    "mmm_calibrated.sample_posterior_predictive(X, extend_idata=True, random_seed=rng)\n",
    "\n",
    "print(\"Posterior predictive samples generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs actual for a sample geo\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Get predictions (in scaled space) and scale back to original space\n",
    "# The model fits to scaled y, so predictions need to be multiplied by target_scale\n",
    "target_scale_uncal = mmm_uncalibrated.get_scales_as_xarray()[\"target_scale\"]\n",
    "target_scale_cal = mmm_calibrated.get_scales_as_xarray()[\"target_scale\"]\n",
    "\n",
    "y_pred_uncal = (\n",
    "    mmm_uncalibrated.idata.posterior_predictive[\"y\"].mean(dim=[\"chain\", \"draw\"])\n",
    "    * target_scale_uncal\n",
    ")\n",
    "y_pred_cal = (\n",
    "    mmm_calibrated.idata.posterior_predictive[\"y\"].mean(dim=[\"chain\", \"draw\"])\n",
    "    * target_scale_cal\n",
    ")\n",
    "\n",
    "# Plot for focus geo\n",
    "geo_mask = df[\"geo\"] == focus_geo\n",
    "geo_dates = df.loc[geo_mask, \"date\"]\n",
    "y_actual = df.loc[geo_mask, \"y\"].values\n",
    "\n",
    "# Uncalibrated\n",
    "ax = axes[0]\n",
    "y_pred_geo = y_pred_uncal.sel(geo=focus_geo).values\n",
    "ax.plot(geo_dates, y_actual, \"k-\", label=\"Actual\", linewidth=2)\n",
    "ax.plot(geo_dates, y_pred_geo, \"--\", color=\"C0\", label=\"Predicted\", linewidth=2)\n",
    "ax.set_title(f\"Uncalibrated Model - {focus_geo}\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.legend()\n",
    "\n",
    "# Calibrated\n",
    "ax = axes[1]\n",
    "y_pred_geo = y_pred_cal.sel(geo=focus_geo).values\n",
    "ax.plot(geo_dates, y_actual, \"k-\", label=\"Actual\", linewidth=2)\n",
    "ax.plot(geo_dates, y_pred_geo, \"--\", color=\"C2\", label=\"Predicted\", linewidth=2)\n",
    "ax.set_title(f\"Calibrated Model - {focus_geo}\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle(\"Predicted vs Actual (Original Scale)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to calibrate a multidimensional MMM using geo-level lift tests:\n",
    "\n",
    "1. **Key Design Principles**:\n",
    "   - Normalize channel data to [0, 1] range for consistent saturation behavior\n",
    "   - Generate data from the model itself using `pm.do`/`pm.draw` to ensure perfect consistency\n",
    "   - Calculate lift tests using the same saturation function the model uses\n",
    "\n",
    "2. **The Problem**: Without lift tests, highly correlated channels are hard to separate\n",
    "\n",
    "3. **The Solution**: Lift test measurements constrain the saturation curve parameters\n",
    "\n",
    "4. **Results**: Calibrated models show:\n",
    "   - Narrower posterior distributions (higher precision)\n",
    "   - Parameter estimates closer to true values (better accuracy)\n",
    "   - Better separation between channel effects\n",
    "\n",
    "### Practical Application\n",
    "\n",
    "In practice:\n",
    "1. **Conduct geo-level experiments** using synthetic control methods\n",
    "2. **Analyze with CausalPy** to get lift estimates (`delta_y`, `sigma`)\n",
    "3. **Format as DataFrame** with columns: `[channel, geo, x, delta_x, delta_y, sigma]`\n",
    "4. **Add to MMM**: `mmm.add_lift_test_measurements(df_lift_test)`\n",
    "\n",
    "### References\n",
    "\n",
    "- [CausalPy Multi-Cell GeoLift](https://causalpy.readthedocs.io/en/latest/notebooks/multi_cell_geolift.html)\n",
    "- [PyMC-Marketing National-Level Lift Tests](mmm_lift_test.ipynb)\n",
    "- [PyMC-Marketing Multidimensional MMM](mmm_multidimensional_example.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc-marketing-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
