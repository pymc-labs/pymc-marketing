# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022-2025, PyMC Labs
# This file is distributed under the same license as the pymc-marketing
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: pymc-marketing local\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-20 21:01+0300\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: es\n"
"Language-Team: es <LL@li.org>\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:10002
#: ae1d6ec93c7e41de988c50973c90576c
msgid "Introducing causal discovery to PyMC-Marketing"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:10004
#: e946394e39de4d0080c92a619e903445
msgid ""
"In marketing, we love to tell stories about *what drives what*: “TV lifts"
" organic traffic,” “Meta drives sign-ups,” “price cuts increase search "
"volume.” Yet those stories rely on an unspoken assumption — that we "
"actually **know** the direction of influence. What if the “driver” is "
"really a downstream effect of something else, like seasonality or product"
" launches? Without a causal map, even elegant regressions or Bayesian "
"models can mistake correlation for causation, leading us to optimize the "
"wrong levers."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:10006
#: e40bf32cb3724bb1826dc1126aeb8b5b
msgid ""
"That’s where **causal discovery** enters: algorithms that learn the "
"underlying graph of cause and effect by testing conditional independences"
" in data. The [classical "
"PC](https://www.jmlr.org/papers/volume8/kalisch07a/kalisch07a.pdf?utm_source=chatgpt.com)"
" and FCI algorithms, used in fields like biology and neuroscience, are "
"brilliant at uncovering causal skeletons under strong assumptions — "
"i.i.d. data, no feedback, and well-separated mechanisms. But marketing "
"rarely plays by those rules. Channels interact, campaigns overlap in "
"time, and KPIs often feed back into exposure decisions. The result? Run a"
" vanilla PC or FCI on raw marketing data, and you’ll likely get nonsense:"
" edges pointing from conversions to impressions, from CTR to spend, or "
"loops that defy business logic."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:10008
#: f005800ff0fd47598c885be52a0d8231
msgid ""
"Does this mean causal discovery is hopeless for marketing? Not at all. "
"Marketing systems are **structural**, not chaotic. We already know many "
"relationships cannot exist — impressions don’t depend on conversions, and"
" every causal path should eventually point toward a measurable target or "
"KPI. We also have strong priors about latent factors like seasonality or "
"market share dynamics. This knowledge drastically **reduces the search "
"space** and makes causal discovery feasible if we incorporate it "
"thoughtfully. That’s exactly what this notebook explores."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:10010
#: 3ddb8cf40ff24060b499ca00653fdc12
msgid "TBFPC algorithm overview"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:10012
#: 4d4e8866735545a38bd4d029223c3c62
msgid ""
"The `TBFPC` algorithm — **Target-first Bayes Factor PC** — is a "
"lightweight, **target-oriented** variant of the classical PC approach, "
"designed with marketing in mind. It tests for independence using **Bayes "
"factors** instead of frequentist tests, and lets you encode **forbidden "
"edges** to reflect domain priors. It adds a **target-edge rule** — "
"`\"any\"`, `\"conservative\"`, or `\"fullS\"` — to bias discovery toward "
"genuine **driver → target** relationships. The Bayes factor is computed "
"via the ΔBIC approximation:"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:10014
#: f49bd40880ec4175a37e54e7c5c742e5
msgid ""
"`TBFPC` is **experimental**, but it bridges an important gap between "
"**Bayesian modeling** and **causal reasoning** for marketing science. It "
"aims not to replace classical discovery frameworks, but to adapt them — "
"respecting the structure, constraints, and realities of marketing data — "
"to yield graphs that make sense both statistically and strategically."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:10016
#: 93bce6adba144eaa9c8c330b73cc4689
msgid "Glossary"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:10017
#: 5f43010c0b104f078d91288866c2c6e3
msgid ""
"Causal discovery – Algorithms that infer directional relationships "
"between variables by testing conditional independences in observational "
"data."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:10019
#: 84ed02c4e24f41999af5fba6a68a185f
msgid ""
"Directed Acyclic Graph (DAG) – A fully oriented, cycle-free graph that "
"encodes one specific causal data-generating process."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:10021
#: 3a5d8863ff984ee09ff32bf9ec166abe
msgid ""
"Completed Partially Directed Acyclic Graph (CPDAG) – A graph with both "
"directed and undirected edges summarizing every DAG in the same Markov "
"equivalence class."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:10023
#: d730c494f1f9451684f29b5cea80d9b4
msgid ""
"Markov equivalence class (MEC) – The collection of DAGs that imply "
"identical d-separation statements and therefore cannot be distinguished "
"via conditional independence tests alone."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:10025
#: 53f318c9bb6b4560ac926325d863aac4
msgid ""
"Target-first Bayes Factor PC (TBFPC) – Target-oriented adaptation of the "
"PC discovery algorithm that applies Bayes factor (ΔBIC) tests plus "
"marketing-specific constraints to learn causal skeletons."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:10027
#: 185b5e484b0b4d5093b441ace9f9c349
msgid ""
"Bayes factor ΔBIC test – The log Bayes factor computed from the change in"
" Bayesian Information Criterion, used to decide whether variables remain "
"dependent after conditioning on controls."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:20002
#: 372c06384182425ba1e654a1ed53c0e7
msgid "Import dependencies"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:50002
#: a4fb4d69e0064fafba10a9b36a938d16
msgid "Setting notebook"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:70002
#: e9fda1e7620c4444a0852ac49135faae
msgid ""
"Let’s begin with a simple but powerful causal story, adapted from an "
"article by **Ben Vincent** — [*“Causal Inference: have you been doing "
"science wrong all this "
"time?”*](https://drbenvincent.github.io/posts/causal_fear.html#fig-"
"causal-dag). In it, Ben illustrates (probably without know it) a common "
"situation in marketing analytics where **search**, **media**, and "
"**sales** are tightly interwoven. We’ll recreate a similar structure "
"here, representing the true (but usually hidden) causal data-generating "
"process."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:70004
#: a53f2748e4994216b96b9af5c82b5612
msgid "Imagine the following system:"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:70006
#: ec309a6bf3744104b85d6c662c32e79e
msgid ""
"People first engage with **generic search** activity (`Q`), such as "
"looking for products or comparing options."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:70007
#: d4d8c4c7106142799bcb23bef10a6817
msgid ""
"This general intent influences **media exposure** (`X`) — those who are "
"searching more are also more likely to see or click ads."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:70008
#: d2f4d89ce4a2423780fe7a48d64d4b48
msgid ""
"Both `Q` and `X` shape **brand search** (`Y`), which captures the "
"strength of the brand in the customer’s mind."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:70009
#: 96687ef2cdf7467eaebd25ae2fc3cabb
msgid "Finally, both `X` and `Y` lead to **purchases** (`P`)."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:70011
#: 63e687b233404bcb8f879fd293e2fd2d
msgid ""
"This graph tells a clear causal story: upstream **intent** (`Q`) drives "
"both **media** and **brand search**, which together drive **sales**. Yet,"
" if we didn’t know this map and simply ran a regression of `P ~ X + Y`, "
"we might overestimate the effect of `X` (media) because part of its "
"variation is confounded by `Q`. Or we might mistakenly interpret `Y` "
"(brand search) as an independent driver of purchases, when in reality it "
"is *mediated* by media and intent. This is the classic trap of confusing "
"*correlation* with *causation*."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:70013
#: 50e1dd9df79d4a0cbfdfd0bec7214034
msgid ""
"Such structures are not hypothetical — they mirror real marketing "
"dynamics. Channels, brand signals, and consumer intent often reinforce "
"each other. Without a causal understanding of how these processes "
"interact, any econometric or Bayesian model risks attributing the wrong "
"effects to the wrong levers. **Causal discovery** gives us a principled "
"way to uncover (or at least approximate) this hidden DAG, helping us "
"build models that explain *why* something happens — not just *what* "
"happens. The next cell visualizes this canonical causal map."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:90002
#: bef14879cf7748b483078595b3fa9550
msgid "We'll now generate data which follow the same causal structure!"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:110002
#: ede6bcf0b8204681a919fef38b7a7d18
msgid ""
"Now that we have our synthetic dataset and its underlying causal "
"structure, let’s see how easily we can recover it using the `TBFPC` "
"algorithm. The class is designed to be simple and intuitive — you just "
"specify the **target variable** (in our case, `purchase`) and the "
"**candidate drivers**. Behind the scenes, the algorithm tests conditional"
" independences using Bayes factors and builds a graph that represents the"
" most plausible causal skeleton consistent with the data."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:110004
#: ca9fa8750b78427c9099323c756064d2
msgid ""
"Under the hood, `TBFPC` begins with a **fully connected graph** between "
"all candidate variables and the target. It then performs a systematic "
"**search for conditional independences**, starting from unconditioned "
"tests (no control variables) and gradually conditioning on larger subsets"
" of other variables. Whenever it finds that two variables are independent"
" given some conditioning set, it **removes the edge** connecting them — "
"this step progressively prunes the skeleton. The process continues until "
"no more edges can be removed under the chosen significance rule."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:110006
#: 22b92dc79fad4ddb9e2d87a37fdb1619
msgid ""
"Each remaining connection represents a dependency that cannot be "
"“explained away” by conditioning on other variables. Directed edges "
"(especially toward the target) are oriented according to the **target-"
"edge rule** you specify, ensuring that the search favors interpretable "
"*driver → target* relationships while respecting any **forbidden** or "
"**required** edges you might impose. The output of this procedure is a "
"**causal graph** — specifically, a **Causal Directed Acyclic Graph "
"(Causal DAG)** or, more precisely, a **Completed Partially Directed "
"Acyclic Graph (CPDAG)**."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:110008
#: 29e3adb0ac344bc2b425d79d43c8e059
msgid "What's the difference between a CPDAG and CDAG?"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:110010
#: 7af4c8d3058743ad8dbd15a5aec1b3c6
msgid ""
"A **Causal DAG** represents a single, fully oriented data-generating "
"structure where every edge has a direction (e.g., `media_activities → "
"brand_search → purchase`). It encodes a complete causal story."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:110012
#: 199d2732571c46389af3bdc67fa088cc
msgid ""
"A **CPDAG**, on the other hand, represents a **Markov equivalence "
"class**: all the DAGs that are statistically indistinguishable from one "
"another given the observed data. It contains both directed and undirected"
" edges — the directed ones are those we can infer confidently from data "
"and logic, while the undirected ones remain ambiguous (they could point "
"either way without contradicting observed independences)."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:110014
#: 01ea4b22d4a14ec68c11410faf0e9011
msgid ""
"With `TBFPC`, obtaining this structure takes only a few lines of code. "
"The algorithm will return the learned graph in **DOT format** (compatible"
" with Graphviz) and you can visualize it or inspect the directed and "
"undirected edges directly. Below we fit the model and print its "
"discovered structure."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:110016
#: e92deaf7bb7645d1b4d444b33978b098
msgid "See how it works!"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130002
#: 130d33d95a814b65bd45883d724bcccb
msgid ""
"The output above shows the **learned causal graph** in Graphviz DOT "
"format. Each line represents a node or an edge, and the syntax captures "
"both **directed** and **undirected** relationships."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130004
#: b5594a95c95f418bb64b3763f5bc9ce9
msgid ""
"Directed arrows such as `\"brand_search\" -> \"purchase\";` or "
"`\"media_activities\" -> \"purchase\";` represent **oriented edges** — "
"causal directions that the algorithm can infer with sufficient confidence"
" based on conditional independence tests. In this case, both "
"`brand_search` and `media_activities` are estimated as **direct causes** "
"of `purchase`, which aligns with the true underlying DAG we simulated "
"earlier."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130006
#: b0b8868b87e9472495424422e0ea332c
msgid ""
"The dashed, undirected edges such as `\"brand_search\" -> "
"\"generic_search\" [style=dashed, dir=none]` represent **ambiguous "
"relationships** — connections that remain *undirected* in the **CPDAG**. "
"The tag `[style=dashed, dir=none]` encodes this explicitly:"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130007
#: 32e826b6d45b49618d44073f30dcc8c3
msgid "`style=dashed` visually marks the edge as *non-oriented*."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130008
#: 2ebf33d000f24d4aa96616e5e163170b
msgid "`dir=none` tells Graphviz to draw the edge **without an arrowhead**."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130010
#: 14d29315792844de925ae33faa1bd153
msgid ""
"Mathematically, this ambiguity means that both possible directions (e.g.,"
" `brand_search → generic_search` and `generic_search → brand_search`) are"
" **Markov equivalent** — they induce the same set of conditional "
"independences in the data. In the language of causal graphs, two DAGs are"
" Markov equivalent if they encode the same **d-separation** relations; "
"that is, for every triplet of variables $(X, Y, Z)$,"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130012
#: 6c56313652f9470c8e719c00ee7635d8
#, python-brace-format
msgid ""
"\n"
"X \\perp Y \\mid Z \\quad \\text{in DAG}_1 \\iff X \\perp Y \\mid Z "
"\\quad \\text{in DAG}_2.\n"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130016
#: bdb7376afa484204a9628b3ca2198e39
msgid ""
"Thus, both orientations would fit the observed independence structure "
"equally well. Without additional assumptions or experimental data, the "
"algorithm cannot determine which direction is correct, so it preserves "
"them as dashed undirected links."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130018
#: c7c9be05333d458a8569b3bc1547d824
msgid ""
"These dashed connections therefore represent the **remaining "
"uncertainty** after conditioning on all other variables up to a given "
"subset size (controlled by `max_k` inside the algorithm). The Bayes "
"factor test behind `TBFPC` compares two competing models for each "
"independence query:"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130020
#: 5ba4b21dad64478fa7fa6ee86add3bde
msgid ""
"\n"
"M_0 : Y \\sim S, \\qquad M_1 : Y \\sim S + X,\n"
"\n"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130025
#: ac121c597da74985bc36ddc189d08b93
msgid "and declares independence if"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130027
#: cf3b06fb13964febb846fa782e208bb7
#, python-brace-format
msgid ""
"\n"
"\\log \\mathrm{BF}_{10} = "
"-\\tfrac{1}{2}\\,[\\mathrm{BIC}(M_1)-\\mathrm{BIC}(M_0)] < \\tau,\n"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130031
#: f16bd4bad93d464ba6b497b6ebf1a3c3
msgid ""
"where $\\tau$ is the chosen Bayes factor threshold. If this inequality "
"holds symmetrically across conditioning sets (i.e., neither direction "
"provides stronger evidence of dependence), the edge remains *ambiguous* —"
" hence, dashed."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130033
#: fc208da7ff624e5d92e2f28668982767
msgid ""
"In other words, dashed edges mean the data support a dependency between "
"variables, but **not enough directional evidence** exists to resolve the "
"arrow. These are precisely the boundaries of the **Markov equivalence "
"class**, and exploring or constraining them (through prior knowledge or "
"further experiments) is the next step toward a uniquely oriented causal "
"DAG."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:130035
#: daac4d7bc2474389b36ead26956806b6
msgid "Let's put this in `Graphviz` to visualize better!"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:150002
#: b0fa52e52d6c44329372744930eca072
msgid ""
"A hard truth in causal discovery is that **the data rarely identify a "
"single “true” DAG**. Many different graphs can encode the **same** set of"
" conditional independences and thus fit the observed evidence equally "
"well. These graphs form a **Markov equivalence class (MEC)** and are "
"summarized by the CPDAG you saw above: solid arrows for orientations we "
"can justify, and dashed links for edges that could point either way "
"without contradicting the tests. In practice, the size of the MEC grows "
"**exponentially** with the number of undirected edges, which is why "
"pinning down *the* DAG from observational data alone is notoriously "
"difficult."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:150004
#: 51dd9be12d6f4ade9078a587a9da0a45
msgid ""
"Still, understanding the MEC is often **enough** for principled "
"decisions. Every DAG in the MEC shares the same d-separation relations "
"(the same testable independences), so they agree on which controls block "
"backdoors and which variables are downstream effects. That means you can "
"design valid adjustment sets, stress-test causal stories, and decide "
"where an experiment or instrument would be most informative — all "
"**without** committing to a single orientation."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:150006
#: ba826f874c7847e2ab3a9adb729309c5
msgid ""
"When you do want to explore concrete candidates, `TBFPC` exposes "
"`get_all_cdags_from_cpdag`. Under the hood, it takes each dashed edge, "
"tries both directions, **filters out cyclic orientations**, and returns "
"the set of fully directed, acyclic graphs consistent with the CPDAG. "
"Formally, it orients each undecided pair $(u,v)$ to either $u\\to v$ or "
"$v\\to u$, and keeps only those orientations for which the resulting "
"adjacency matrix is acyclic. This gives you a menu of plausible worlds "
"that all fit the evidence."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:170002
#: 311c69b9dcfa4771913520eb641e9bf4
msgid ""
"What should you do with that menu? Use **business sense** and **model "
"diagnostics** (We'll talk about build models from dags soon) to pick the "
"DAG that best aligns with mechanism knowledge (e.g., “impressions don’t "
"depend on conversions”) and validates empirically (posterior predictive "
"checks, out-of-sample performance, robustness to adjustments)."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:190002
#: 0c225ceb226d48d7960eed47d4000ec9
msgid ""
"The panel plotted shows **every fully oriented DAG** that is consistent "
"with the CPDAG we learned: each dashed link has been assigned a "
"direction, cycles were filtered out, and what remains are alternative "
"causal stories that all **fit the same independence structure**. Although"
" the arrows now point one way or the other in each subplot, these DAGs "
"are still members of the **same Markov equivalence class (MEC)** — they "
"share the same skeleton and the same set of **unshielded colliders** "
"(v-structures), and therefore imply the **same d-separations**. In other "
"words, for any triple $(X,Y,Z)$ and any conditioning set $S$, we have $X "
"\\perp Y \\mid S$ in one DAG **iff** it holds in all DAGs in the panel."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:190004
#: e7c5c470b56144dea644b76de1f431c4
msgid ""
"Here we can do two things: (A) ask whether each of the recovered DAGs "
"lies in the **same MEC** as the *true* structure, or (B) ask whether the "
"**CPDAG** we discovered represents that same equivalence class. In real-"
"world marketing problems, we can’t perform either check — the “true DAG” "
"is unknown. That’s precisely why causal discovery is useful: it narrows "
"down the space of plausible causal worlds and makes explicit *where "
"uncertainty remains*. The decision of which DAG to trust must then come "
"from a blend of **data evidence** and **domain judgment** — what you "
"believe is causally reasonable given how your business and campaigns "
"actually work."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:220002
#: 20a066d3750b44cf81fc1d251c8e40e5
msgid ""
"Here, we can see the class was able to recover the **True DAG** and all "
"Causal DAGs from the CPDAG, lie in the same MEC. Let's move to another "
"example, and see how the class perform if we change the data genaration "
"process (DAG under the hood)."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:220004
#: ff245572cd9c4f3ab5e7386f6e7feb3a
msgid ""
"Let’s make things a bit more interesting. The next example represents a "
"new causal world — a slightly different **marketing mechanism** that will"
" challenge our discovery procedure. This time, imagine that **generic "
"search** (`A`) drives **media activities** (`B`), which in turn boost "
"**brand search** (`C`), ultimately leading to **purchases** (`P`). "
"Alongside this main pathway, an **exogenous factor** (`D`) — think of it "
"as the broader economy, market trends, or seasonality — also directly "
"influences purchases, independent of the marketing funnel."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:240002
#: 9decefb5104e4cc78f0ea97c65ccafa1
msgid ""
"This structure tells a familiar marketing story: awareness and "
"consideration flow through a funnel (from search intent to media exposure"
" to brand lift to sales), while external forces like macroeconomic "
"conditions add background variation that we can’t fully control. It’s an "
"elegant but subtle system, because now two causal paths meet at the "
"purchase node: one endogenous (marketing-driven) and one exogenous "
"(context-driven). Detecting that distinction is exactly the kind of "
"challenge causal discovery is meant to address."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:240005
#: 58d81688cbbd43c486f0c492a9e07297
msgid ""
"All the examples in this notebook are synthetic. They are intentionally "
"designed to stress-test the algorithm and illustrate different causal "
"motifs — some plausible in marketing, others less so. The goal isn't to "
"claim these graphs are \"true,\" but to show how the `TBFPC` class "
"behaves under varied structures, and what kinds of insights (or pitfalls)"
" each configuration can reveal."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:240008
#: e38851dda86e482d8ea716e7e393789e
msgid "As before, we need to generate data with this generative structure."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:260002
#: 9c20f9baad3240728538cdbe4068862d
msgid "Could we find out the true DAG now?"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:280002
#: a562e539d6fb46a89ad2c0ea73cb3d79
msgid ""
"Beautiful — the class successfully recovered the **true causal "
"structure** from the data, meaning that the CPDAG it discovered lies in "
"the same **Markov equivalence class** as our simulated DAG. In other "
"words, the algorithm correctly identified the backbone of the causal "
"system: the sequential flow from **generic search → media → brand → "
"purchase**, plus the independent influence of the **exogenous** factor on"
" purchases."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:280004
#: 1a2db5145ff54a7c9c9f246e85279c4b
msgid ""
"This outcome highlights the **efficiency and reliability** of the `TBFPC`"
" approach for small, well-structured problems. Even without access to the"
" true underlying DAG, the algorithm can reconstruct a consistent causal "
"skeleton from observational data alone — a strong indication that the "
"combination of **Bayes factor–based independence testing** and **target-"
"oriented edge rules** is working as intended."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:280006
#: 74883df01fa7459f94ee4db172eeb932
msgid ""
"Of course, these are still **simple synthetic systems** — clean, noise-"
"controlled, and designed to behave nicely. Real-world marketing data are "
"messier: time dependencies, measurement errors, latent effects, and "
"correlated interventions all complicate causal inference. So let’s keep "
"raising the difficulty and see how the algorithm performs when the "
"structure becomes less ideal and closer to what we face in practice."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:280008
#: 9f2ae719f94f440cbe9a3dcc9a8300c0
msgid ""
"Let’s raise the bar again — this time with a more complex and realistic "
"causal system. Imagine a marketing ecosystem where **generic search** "
"(`A`) acts as a top-of-funnel signal: it fuels both **media activities** "
"(`B`) and **brand search** (`C`), as consumers who are already searching "
"generically become more likely to see ads and later perform branded "
"queries. In turn, **media** also influences **brand search**, reinforcing"
" brand awareness and familiarity. Finally, **purchases** (`P`) are driven"
" jointly by all three — media exposure, brand strength, and the influence"
" of an **exogenous factor** (`D`), which could represent macroeconomic "
"conditions, promotions, or competitor activity."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:300002
#: aa4950ee29fb4f24ad92d6a046690788
msgid ""
"This setup is much closer to the reality faced by marketing analysts: "
"overlapping effects, multiple paths to the same KPI, and unobserved "
"influences lurking in the background. Notice that now **both media and "
"brand search** are children of generic intent (`A`) *and* parents of "
"purchase (`P`). This creates a web of **mediated and confounded "
"effects**, which are notoriously hard to disentangle with standard "
"regression approaches — even if all the variables are observed."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:320002
#: f442aa5818d64c6fbcd0dda56418c754
#, python-brace-format
msgid ""
"So far, our three setups showcased the classic causal motifs that matter "
"in marketing: **chains** (driver cascades like `generic_search → media → "
"brand → purchase`), **forks** (shared causes like `exogenous → {purchase,"
" other signals}` that create confounding), and **colliders** (converging "
"arrows like `media → brand ← intent`, which stay *blocked* unless you "
"condition on them or their descendants)."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:320004
#: 2a4bd021d3e1403e9ffbaa4fa4294939
msgid ""
"In each case, `TBFPC` handled the essentials: it pruned the skeleton "
"where conditional independences existed, oriented **driver → target** "
"edges when the Bayes evidence supported them, and left ambiguous links "
"dashed (the CPDAG), faithfully reflecting the MEC."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:320006
#: 53de15bdc4f24a8aa398635a27edfda0
msgid "Now look at the next structure, which is trickier:"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:320008
#: ef1a1692805741bbbfbcd4e8ad431c13
#, python-brace-format
msgid "`D → A`, `A → {B, C}`, `B → C`, and `D → C`, with `{B, C} → P`."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:320009
#: e0267f01f3af4cb8a82eb3d474fb800c
msgid ""
"Here, **C is a multi-parent collider** (`A → C ← D`) *and* also a child "
"of `B`."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:320010
#: 29c50d8c1f7a49a6a516bd40f5e71f8b
msgid "`D` plays a **fork/confounder** role for both `A` and `C`."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:320011
#: 6a78f0a53b0f4426a23665e978d856a8
msgid ""
"Paths like `D → A → B → C` coexist with the direct `D → C`, creating "
"**redundant routes** and tight correlations among predictors."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:320013
#: 9530df2aa29a4146a6757108d0588be5
msgid "Why might `TBFPC` struggle at first pass?"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:320015
#: 83a0e634450542869e0d81f501c7df88
msgid ""
"**Collider opening during testing.** Because we test many conditioning "
"sets, including `C` (or its descendants) in a conditioning set can *open*"
" paths (e.g., between `A` and `D`) and make variables look dependent, "
"blocking edge removals that should occur. Classical PC mitigates this "
"with orientation phases and collider rules; our class intentionally "
"**does not run Meek rules**, so it’s easier to keep extra edges in the "
"skeleton when colliders are involved."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:320017
#: 9a3f0c6a19f64ee4b343c75ab06d714b
msgid ""
"**Redundant pathways and near-unfaithfulness.** Multiple parallel routes "
"into `C` (via `A`, `B`, and `D`) can produce strong collinearities. With "
"finite samples and noise, ΔBIC CI tests may lack power to find a "
"separating set (especially when signals partially cancel or reinforce), "
"so edges stay."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:320019
#: 03d3b62527214e9db3fcfc94d46848d5
msgid ""
"**Target-first orientation only.** We only bias arrows **into the "
"target**; orientations among drivers (`A, B, C, D`) are deliberately "
"conservative. This preserves correctness at the cost of leaving more "
"dashed links, and sometimes an **over-connected** skeleton, on complex "
"driver subgraphs."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:320021
#: 1edfd3ef87be4a29bae3834765975332
msgid ""
"The good news: we can **guide** the procedure using built-ins. In the "
"next step we’ll (i) encode domain priors with `forbidden_edges` (e.g., "
"“purchases don’t cause media”), (ii) lock essential mechanisms with "
"`required_edges`, and (iii) tighten how we treat target edges via the "
"`target_edge_rule` (e.g., `\"fullS\"` for stricter retention). These "
"levers constrain the search space, protect collider structures from being"
" inadvertently “opened” during testing, and help the algorithm converge "
"to a sparser, more faithful CPDAG that matches the marketing story you "
"actually believe."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:350002
#: f8110dafc1a14a058f4dfeab76c7ff9f
msgid ""
"**What went wrong (and why):**   The procedure correctly oriented the "
"**target edges** — `brand_search → purchase` and `media_activities → "
"purchase` — but it **under-oriented (left dashed)** several "
"**driver–driver** relations and even **dropped** one adjacency that "
"exists in the true structure:"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:350005
#: 333badfc6e30402f9e3b984fdd6ca32e
msgid ""
"**Missing edge:** there is **no** `media_activities — brand_search` link "
"in the learned CPDAG, even though the true DAG has `media_activities → "
"brand_search`. This is a **false negative** in the skeleton."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:350007
#: dd7dcfecb2e34798b5b8a4facd035d8a
msgid "**Undirected (dashed) edges where the true DAG is directed:**"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:350008
#: f4d4a243d1334a9d93db563dde027cde
msgid "`exogenous — generic_search` (true: `exogenous → generic_search`)"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:350009
#: 2edca4a96e2f4d3f9782e19abfc4a449
msgid ""
"`generic_search — media_activities` (true: `generic_search → "
"media_activities`)"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:350010
#: 1bb124e4fcf049a0b55cc2b9020570b0
msgid "`generic_search — brand_search` (true: `generic_search → brand_search`)"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:350011
#: 84069bef8d34435fbb60df05c0c5813a
msgid "`exogenous — brand_search` (true: `exogenous → brand_search`)"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:350013
#: fbe309982c9c45859fd51aec98b34cc5
msgid ""
"Why can this happen? (i) **Collider handling**: conditioning on (or on "
"descendants of) a collider can spuriously create dependence, preventing "
"the removal of edges you’d expect; we don’t run Meek rules, so we keep "
"more ambiguity. (ii) **Redundant paths & collinearity**: with routes like"
" `exogenous → generic_search → media_activities → brand_search` "
"coexisting with direct `exogenous → brand_search`, ΔBIC tests may "
"struggle to find separating sets at finite samples, so edges are either "
"**left undirected** or **pruned incorrectly**. (iii) **Target-first "
"bias**: orientations among drivers are conservative by design; we "
"prioritize reliable `driver → target` arrows and leave driver–driver "
"directions unresolved unless the evidence is strong."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:350015
#: 5f7e93f1e1a642ddaf6b8128f4b8daed
msgid "How to guide the algorithm with domain knowledge:"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:350016
#: f336eacfb81c4bd7ac90a249318b6c47
msgid ""
"Marketing gives us strong, defensible priors. Use them to *constrain the "
"search space*:"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:350018
#: f5845dcf44d548e6b0bf2b0674dbe1a5
msgid "Choosing the target-edge rule:"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:350020
#: 83b19223f7564803b6a34628c2699ae3
#, python-brace-format
msgid ""
"*\"any\"* (more aggressive pruning): keep X → target unless any "
"conditioning set makes $X \\perp \\text{target} \\mid S$. **Pros**: "
"precise, trims spurious target links.  **Cons**: can drop true drivers "
"when signals are weak or near-unfaithful (higher false negatives)."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:350024
#: 6fe18e5541e04bdc97debeb1f147ad88
msgid ""
"*\"conservative\"* (more protective): keep X → target if at least one "
"conditioning set shows dependence. **Pros**: better recall of true "
"drivers (fewer false negatives); robust when paths are redundant.  "
"**Cons**: may keep some spurious links (denser graph)."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:350028
#: 8638c29e5a624ab5951a9d1035f4ed5f
msgid ""
"*\"fullS\"* (stability via maximal conditioning): test only with the full"
" set of other drivers as $S$. **Pros**: fewer arbitrary choices of $S$; "
"stable behavior.  **Cons**: risks over-control (blocking mediators) or "
"opening colliders if descendants sit in $S$; may under-orient."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370002
#: e640effc678446b6be2e6092090790da
msgid ""
"Switching `target_edge_rule` from `\"any\"` to `\"conservative\"` changes"
" **how edges into `purchase` are retained**. With `\"any\"`, an edge like"
" `media_activities → purchase` (or `brand_search → purchase`) is "
"**removed** if there exists *any* conditioning set `S` such that the data"
" support independence between that driver and `purchase` given `S` "
"(aggressive pruning). With `\"conservative\"`, the edge is **kept** if "
"**at least one** conditioning set shows dependence (protective "
"retention). In other words:"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370004
#: 800816d307cf4359a5e0e49752303a98
#, python-brace-format
msgid ""
"`\"any\"`: delete `driver → purchase` if there **exists** `S` with $\\log"
" \\mathrm{BF}_{10}(\\text{driver} \\to \\text{purchase} \\mid S) < "
"\\tau$."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370006
#: de929095c47847d5b356a3ede8b0706f
#, python-brace-format
msgid ""
"`\"conservative\"`: **keep** `driver → purchase` if there **exists** `S` "
"with $\\log \\mathrm{BF}_{10}(\\text{driver} \\to \\text{purchase} \\mid "
"S) \\ge \\tau$."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370008
#: 05ffaecb5cac46e89288d584ee111410
msgid ""
"In our new CPDAG, the **target edges remain oriented** — `brand_search → "
"purchase` and `media_activities → purchase` — which is exactly what "
"`\"conservative\"` encourages: there is at least one conditioning set "
"where each driver still shows dependence with `purchase`. The "
"**driver–driver** relations remain **dashed (undirected)**:"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370010
#: 62e425859d9e451abb5a024cf46f4a92
msgid "`brand_search — exogenous`"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370011
#: c58dc5f3b95d4794ac718ad871ad7fe7
msgid "`brand_search — generic_search`"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370012
#: a46934cfd9c34172a608ca66a78cd155
msgid "`exogenous — generic_search`"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370013
#: 1878df844d1a4dc3bedefba436c172eb
msgid "`generic_search — media_activities`"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370015
#: 3e64879c15da46cca20f3ceb4ebef681
msgid ""
"These stay undirected because the target-edge rule **does not orient "
"edges among drivers**. Their directions are **Markov-equivalent** given "
"the observed independences, so the algorithm leaves them unresolved."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370017
#: ac2c85000d1e41ab97d2219921d4194c
msgid ""
"**How consistent is this with the true DAG?** It’s closer on the *target "
"side* (we retain the true causes `brand_search` and `media_activities` of"
" `purchase`) but still *under-oriented upstream*. The true structure "
"favors directions such as `exogenous → generic_search`, `generic_search →"
" media_activities`, and paths into `brand_search` (`generic_search → "
"brand_search`, `media_activities → brand_search`)."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370019
#: 713382fc9b9849daa5fedf4f611efe9f
msgid "It's time to bring general knowledge."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370021
#: ed7b8d78d4e64e2eafebaf954c8fc588
msgid "External knowledge from the ecosystem:"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370023
#: c2bb4bf2e2374e23899ab81a75d0e33b
msgid ""
"It is rarely sensible that brand search causes exogenous conditions or "
"generic intent; your experience strongly favors the opposite. Encoding "
"those as required directions above reflects the mechanism you actually "
"believe."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370025
#: d7c4bec848a641b3b274f01ceb422c5b
msgid ""
"You likely expect `generic_search → media_activities` and some path from "
"media to brand (creative/awareness effects), so compel `media_activities "
"→ brand_search` rather than letting it vanish."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:370027
#: 53d81850738d4dc5ae60246e89dd17c1
msgid "Let's use this information now!"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:390002
#: e7d697ae611346539301b0f453296cc6
msgid ""
"Success — with a small dose of **domain knowledge** we’ve steered "
"discovery to the **true structure**. By *forbidding* implausible links "
"into `purchase` (`generic_search`, `exogenous`) and *requiring* core "
"mechanisms (`media_activities → brand_search`, `exogenous → "
"brand_search`), we constrained the search space so the Bayes-factor CI "
"tests could focus on what’s causally plausible. This is the intended "
"workflow: start from a CPDAG learned from data, then iteratively **inject"
" priors** (forbidden/required edges, target-edge rule) to refine the "
"graph into a story that is both **statistically supported** and "
"**business-sensible**."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:390004
#: 397ecb62e5e44f029017f5260e0a3f70
msgid ""
"As always, remember that a CPDAG represents a **Markov equivalence "
"class**. Even when we recover the true structure, there may be other DAGs"
" that fit the same conditional independences. If you want to audit that "
"space, enumerate all consistent DAGs (e.g., `get_all_cdags_from_cpdag`) "
"and sanity-check them against mechanism knowledge and validation "
"diagnostics. The goal isn’t dogma about a single graph, but a **reliable "
"causal narrative** that survives both the data and your understanding of "
"how marketing actually works."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:410002
#: 9017846f22c0484b88b50bcebe73da05
#, python-brace-format
msgid ""
"We’ve stress-tested `TBFPC` on tidy, cross-sectional toy worlds. Now "
"comes the real beast: **time series**. Marketing data arrive as sequences"
" — budgets, impressions, search, conversions — with **autocorrelation**, "
"**seasonality**, and **policy feedback** (yesterday’s performance shapes "
"today’s spend). In such settings, plain “snapshot” conditional "
"independences can mislead: serial correlation shrinks the *effective* "
"sample size, nonstationarity makes relationships drift, and lags create "
"**mediated paths over time** (e.g., `media_{t-2} → brand_{t-1} → "
"sales_t`). Even worse, conditioning on certain lags can **open "
"colliders**, while failing to include them can leave **backdoors** "
"unblocked."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:410004
#: e65fb08df2244cfdacd227a28d23d5a0
#, python-brace-format
msgid ""
"Why is causal discovery harder in time? First, many DAGs that differ in "
"**instantaneous** directions are **Markov equivalent** once you add "
"strong serial dependence; their d-separations look the same after you "
"include common trend/seasonal drivers. Second, **feedback** (`sales_{t-1}"
" → spend_t`) violates the DAG acyclicity if modeled at a single time "
"index — the “arrow of time” must be enforced across lags. Third, "
"aggregation and measurement delays blur timing, so $X_t$ may appear "
"contemporaneously related to $Y_t$ even if the true effect is $X_{t-1} "
"\\to Y_t$. Granger predictability is *not* causality; it’s necessary but "
"far from sufficient."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:410006
#: 28cb4dff362348f29cc7a10e7d7c51f1
msgid "Capturing Time series CPDAGs"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:410008
#: 6a742865bc264fbd9a38d97fa045f257
msgid ""
"Before diving into time-dependent causal graphs, we need a way to "
"**simulate realistic marketing time series** — data that move gradually, "
"not randomly jump around. For that, we’ll use a simple yet powerful "
"utility: a **bounded random walk**."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:410010
#: 44775d43bb6e4d7bbc65e9a7ffeb66dc
msgid ""
"The function below, `random_walk`, generates a sequence of values that "
"evolve step-by-step with small stochastic increments around a target mean"
" (`mu`) and standard deviation (`sigma`). You can also specify lower and "
"upper bounds, ensuring that values stay within a realistic range — for "
"example, between 0 and 1 for normalized ad spend, or within a limited "
"percentage range for brand search interest. The result is a smooth, "
"autocorrelated trajectory that mimics how signals such as spend, "
"impressions, search volume, conversions, etc. behave over time."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:410012
#: 8296329b32594b5f82f8a858d93ae09b
#, python-brace-format
msgid ""
"This kind of controlled stochasticity is crucial for our next "
"experiments: it gives us **time-correlated inputs** that create temporal "
"dependencies between variables (e.g., `media_{t-1} → brand_{t} → "
"sales_{t+1}`) without exploding variance or unrealistic oscillations. In "
"other words, this random walk helps us *simulate the inertia and memory* "
"inherent in marketing processes — the very features that make **time-"
"series causal discovery** both fascinating and challenging."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:430002
#: a11af898bb3948a689b6734aeef19d22
msgid "Let's define a simple DAG structure!"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:450002
#: 21b8e48ba3f44e22bb88dacf520836a7
msgid ""
"But now, lets populate with time series each of the factors for the "
"target variable!"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:470002
#: 7f7d50eba497487a8fa12ad917220fe4
msgid "What our model has to say? it could find the right structure?"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:490002
#: 61403b35ca18411683ef57cee04c5f97
msgid ""
"Why did this simple time-series case “fail”? Even though the **true graph"
" is contemporaneous** (`generic_search`, `media_activities`, "
"`brand_search`, `exogenous` all point into `purchase` at the same time "
"index), the data generated are **strongly autocorrelated**. "
"Autocorrelation makes predictors move together over long stretches, "
"creating **near-collinearity** and **spurious contemporaneous "
"associations**. In that setting, conditional-independence tests on $Y_t "
"\\sim \\{X_{1,t},X_{2,t},X_{3,t},X_{4,t}\\}$ can’t tell whether a signal "
"is a genuine *instant* driver or just a **proxy for past values** (e.g., "
"$X_{1,t-1}$) or a shared trend. Worse, the ΔBIC Bayes-factor uses $n$ in "
"its penalty; when series are autocorrelated the **effective sample size**"
" is much smaller, so evidence is **overstated/understated** in ways that "
"can flip marginal decisions near the threshold."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:490004
#: 2f0143e7bf364ab7a8f119f955f79a2c
msgid ""
"In short: with time series, the right graph is rarely “flat and "
"contemporaneous.” Real mechanisms are **lagged** (carryover/adstock), and"
" shared temporal structure (trend/seasonality) acts like **latent "
"confounding**. If we omit lags and temporal controls, the CPDAG will (i) "
"keep extra dashed links among drivers, (ii) miss true edges (low power "
"after partialling correlated series), or (iii) orient target edges "
"inconsistently depending on which conditioning sets soak up more "
"autocorrelation."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:490006
#: 77d9896860064605a230e33efed77cb7
msgid "**How to handle it?**"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:490008
#: 1d0f7407190145fdba6601e61093c9f5
msgid ""
"Adding one–period lags (`x1_t1`, `x2_t1`, …) isolates **within–variable "
"temporal persistence**—each series’ own momentum—while forbidding cross-"
"lag edges prevents spurious links between drivers that merely co-move "
"through shared trends or autocorrelation. In the original contemporaneous"
" setup, predictors like `brand_search` and `media_activities` rose and "
"fell together, so conditional-independence tests could not tell whether "
"an observed effect on `purchase` was causal or just inherited from their "
"joint temporal drift."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:490010
#: 422f80508385479eb358909d0b2bd26d
msgid ""
"Introducing each variable’s lag lets the model partial out that internal "
"memory: the lag explains most of the slow component, leaving the residual"
" (innovation) closer to the *instantaneous* causal signal. By forbidding "
"edges among the lagged features, we ensure that these lags act only as "
"**self-controls**, not as new confounders across channels. This simple "
"configuration effectively de-trends and de-autocorrelates the drivers, "
"stabilizing the Bayes-factor tests and clarifying which contemporaneous "
"edges into `purchase` remain after accounting for each series’ own past. "
"It is therefore a sound minimal strategy for time-series causal "
"discovery—lightweight, interpretable, and providing a first approximation"
" to the dynamic structure before introducing richer lag networks or "
"seasonal terms."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:510002
#: af3418b4072045e5b19d2c3ddb73e0f6
msgid ""
"Brilliant — the new DAG captures exactly the dynamic structure we were "
"aiming for."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:510004
#: 63bc1c856d7d4f7c959bfe3faea73349
msgid ""
"After introducing the two–period lags, the algorithm correctly recovered "
"that all four contemporaneous drivers (`generic_search`, "
"`media_activities`, `brand_search`, and `exogenous`) directly influence "
"`purchase`, while the lagged variables appear only as their natural "
"temporal continuations (`generic_search` → `x1_t2`, `media_activities` → "
"`x2_t2`, etc.). The presence of these dashed self–lag links simply "
"reflects each series’ own persistence, not new causal pathways."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:510006
#: b708daff0d2948038e843f260a41c0be
msgid ""
"If we conceptually remove the lag nodes—treating them as background "
"controls that absorb autocorrelation—the remaining contemporaneous graph "
"between the main drivers and `purchase` matches the **true causal "
"structure** almost perfectly. In other words, by adding minimal temporal "
"structure we forced the model to explain away autocorrelation through "
"each variable’s own past, which freed the discovery step to reveal the "
"genuine contemporaneous causal pattern we originally encoded. This is a "
"clean and interpretable confirmation that the lag–augmented approach "
"brings the recovered DAG much closer to the underlying truth."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:530002
#: 159308c02ed04db38bd1112473ab4bf1
msgid "How does this work for longer structures? Let's continue testing!"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:550002
#: a1bed689c4004f66bf0d709390133006
msgid ""
"This structure is similar to the previous expose. Lets reply the same "
"process!"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:570002
#: 4b148d84eefe4a25a178ab2f476ff881
msgid ""
"Once, again we got something similar to the original but this time, as "
"before we could not refine the dag with our domain knowledge to get "
"something as expected."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:590002
#: e2c23c9a6f26419ba5b3dfd78a50f55f
msgid ""
"Brilliant — once again, the new DAG captures exactly the dynamic "
"structure we were aiming for. After cleaning the lags variables, the DAG "
"should be in the same MEC."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:610002
#: 0cc5ee508ed9428baf0fe1d6d8d42f70
msgid ""
"Amazing! Having reached a coherent and time-respecting causal structure, "
"we are now ready to move from **discovery** to **estimation**."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:610004
#: 6c50520fd4004788963f5f455225a360
msgid ""
"The next step is to quantify the strength of the causal relationships "
"encoded in our DAG — meaning, to move from the *graph* to the *estimands*"
" that describe how much each driver truly contributes to the target."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:610006
#: 1eb4a093df1d4ecf993fbd4e2bba3441
msgid "Building models from DAGs - Full luxury causal Bayesian approaches!"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:610008
#: 62f7bf7454704731817e427fa3145f15
msgid ""
"We introduce the class **`BuildModelFromDAG`**, a bridge between the "
"causal graph and a fully specified Bayesian model. Once the structure is "
"established (for example, through `TBFPC` or any other discovery "
"algorithm), `BuildModelFromDAG` automatically translates that structure "
"into a probabilistic program where each directed edge becomes a candidate"
" causal path, and each node’s parents define its conditioning set. This "
"approach allows us to fit all relationships **jointly**, preserving "
"uncertainty propagation across the entire system rather than estimating "
"each edge through separate regressions."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:610010
#: 7d073b957a614fe39f5979a3ac1bc5fb
msgid ""
"Conceptually, this is a “full-luxury” causal-Bayesian workflow: instead "
"of identifying adjustment sets via the classical back-door criterion (as "
"we explored in our previous post [*Causal Identification in "
"MMM*](https://www.pymc-"
"marketing.io/en/stable/notebooks/mmm/mmm_causal_identification.html)) and"
" fitting many conditional models, we let the DAG directly determine the "
"hierarchical dependencies inside a single Bayesian model. The result is a"
" unified inference of all estimands — posterior distributions for each "
"causal effect — consistent with the discovered structure and ready to "
"integrate with marketing mix modeling or other decision frameworks."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:610012
#: 0a142646e7c8462088d91423bf32601e
msgid ""
"In short, while the previous section focused on *which* arrows exist, "
"`BuildModelFromDAG` now tells us *how strong* those arrows are. It "
"completes the transition from causal discovery to causal quantification, "
"enabling an end-to-end pipeline from raw data to interpretable, "
"uncertainty-aware causal estimates."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:630002
#: c80408c3d20b48118793b3bd04d61ab0
msgid ""
"The class works with causal dags, not with CPDAGs. We can use our "
"function get the different Causal DAGs which lie in the same MEC "
"discovery by the algorithm, and now, use those to build a model."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:660002
#: eb185d873d0b4c508a895685006f8bb2
msgid ""
"Great, with this few lines we have a pymc model which replicates our "
"precise causal structure. Let's see the computational graph derive from "
"the causal graph."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:680002
#: 43020760b3e94a0794e8fee57acdcce6
msgid ""
"Great, if your computational graph follows our causal structure, then we "
"can sample and get easily the true coefficients of each variable over "
"purchases."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:700002
#: 15f7bd96a507491fa913985f44a5dbbd
msgid ""
"As you can see, once the model is built you can treat it as any other "
"PyMC Model."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:720002
#: 07ba5768d20e4bfdb255dcddceec3e22
msgid ""
"Could we recover the true effects? - Indeed, let's see the following "
"picture!"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740002
#: 0412b3aa200b4c669c2ce4399f651924
msgid ""
"Excellent — the estimated posterior means align closely with the true "
"underlying coefficients, confirming that our Bayesian reconstruction "
"faithfully captured the causal signal embedded in the data."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740004
#: e218104c9f4c4711b1e5c41b5adeafb3
msgid ""
"What is particularly powerful about this approach is its simplicity: "
"rather than building and fitting multiple regression models — one per "
"effect or per adjustment set — we recover all estimands "
"**simultaneously** within a single coherent probabilistic model. Each "
"parameter reflects its causal interpretation directly from the DAG, and "
"uncertainty is propagated through the full system rather than estimated "
"piecemeal."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740006
#: face4c8c9d22482cb1dafbdcab9c0bff
msgid ""
"In this example, the graph served as the blueprint, the Bayesian model "
"performed the joint estimation, and the result is a unified, transparent "
"view of how each driver contributes to the outcome. This not only saves "
"modeling effort but also ensures internal consistency — all effects are "
"inferred together under the same causal and probabilistic assumptions."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740009
#: 6b989c7e32704efab211fbb1466d4d53
msgid "**Causal discovery caveats**"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740011
#: 0ee9b932e60742b69f03735723ae6528
msgid "All causal discovery methods rely on strong assumptions:"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740013
#: d82489581f3a4236bb729c79c452575c
msgid ""
"(i) The data-generating process is Markov and faithful to a DAG so "
"d-separations line up with conditional independences. (ii) The measured "
"variables capture every relevant cause (no hidden confounders) (iii) "
"Samples are i.i.d. or at least independent enough that independence tests"
" behave properly. (iv) The finite-sample tests are powerful enough to "
"separate signal from noise."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740018
#: 5efa6c3165bf4979be0ac916c66fd4fb
msgid ""
"The classical PC algorithm assumes causal sufficiency (no latent common "
"causes) and acyclicity; it removes edges via conditional-independence "
"tests and orients the remainder with Meek’s rules, so any violation of "
"faithfulness or weak tests can leave extra or missing edges. FCI relaxes "
"causal sufficiency by allowing latent variables and selection bias, but "
"in return it produces a richer set of partially oriented edges "
"(bidirected, circle endpoints) that still require strong assumptions to "
"interpret. Because real marketing datasets often include feedback loops, "
"non-stationarity, measurement error, and latent drivers, we treat these "
"algorithms as hypothesis generators rather than truth oracles and bake in"
" domain constraints wherever possible."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740021
#: ddc66e8a6213448f875ed21699decc45
msgid "MMM modeling workflow"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740023
#: b91ae9485c244aac8ed91fca2f59f034
msgid ""
"If you are thinking how this goes into the workflow to develop a model, "
"you can visualize this as start with causal discovery, identify a "
"credible DAG in a MEC consistent with your data, then quantify by two "
"optional ways."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740025
#: e144390b183c47d38a46ca5b3f012959
msgid ""
"Causal identification route: Once the CPDAG gives you a candidate driver "
"set for the chosen target, drop that structure into the "
"`pymc_marketing.mmm.causal` model. It conditions on exactly the back-door"
" adjustment set implied by the DAG, so you estimate each driver’s causal "
"effect with targeted regressions or MMM variants. Great for stress-"
"testing multiple DAGs and comparing estimands side by side."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740027
#: c2923b9833e64ff0bfcc3bd191cb0661
msgid ""
"Full DAG-to-model route: Feed a fully oriented DAG into "
"`BuildModelFromDAG` to spin up a single PyMC model that respects the "
"entire causal graph. Each edge gets a slope, the target retains its "
"likelihood, and you sample the joint posterior to read off linear causal "
"effects directly. This is ideal when you want one coherent Bayesian model"
" rather than separate conditionals."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740030
#: a2084ff694ee499a9ba4c36b8f29802a
msgid ""
"Soon we’ll extend `BuildModelFromDAG` so you can plug in nonlinear "
"response transforms (e.g., saturation, adstock) along individual edges. "
"That will let you stay in the DAG-driven workflow while capturing "
"diminishing returns or channel-specific dynamics right inside the "
"generated PyMC model."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740033
#: 370d0707aaad4abe91c39c17d0617110
msgid "Conclusion"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740035
#: 3607898c84754dc58856b0b7f4337e46
msgid ""
"Through this complete workflow — from causal discovery to Bayesian "
"estimation — we demonstrated how structural reasoning and probabilistic "
"modeling can be unified into a single, coherent framework."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740037
#: c25323c0f482428bbf8d112260ab1114
msgid ""
"Starting from raw data, or autocorrelated time-series data, we first "
"learned that discovering a purely DAG is unreliable for different "
"reasons. By introducing domain knowledge and/or explicit time-order "
"constraints, we corrected and avoided spurious connections, allowing the "
"graph structure to reflect true causal direction."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740039
#: 1e4c62d00416485e905a396ac5697d5b
msgid ""
"Once a stable DAG emerged, we transitioned from structure to substance: "
"using `BuildModelFromDAG`, we converted the discovered causal "
"relationships into a fully specified Bayesian model. This let us estimate"
" all causal effects jointly — obtaining posterior distributions for every"
" edge without needing to build a patchwork of separate regressions. The "
"resulting estimands closely matched the true underlying coefficients, "
"validating both the causal design and the Bayesian inference process."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740041
#: ac33eb9b2c194c848041b42388bce785
msgid ""
"Ultimately, this workflow illustrates the strength of an integrated "
"causal–Bayesian approach:"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740043
#: 2abaf49d64c040d4b3216fda835f742c
msgid "**Causal discovery** provides the structure (who influences whom)."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740044
#: 4dc2e9f1eac248cdb54b9fa4158eb93b
msgid ""
"**Bayesian estimation** provides the magnitude (how strongly, and with "
"what uncertainty)."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740046
#: 43e87a7f6ee04ed4adf37e8ea1e89ff6
msgid ""
"Together they form a complete loop — from learning the graph, to "
"quantifying its effects, to validating against reality."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740048
#: 1bc747c0f2f146f8b979e0ca4ac97f88
msgid ""
"While no method is immune to confounding or model misspecification, this "
"pipeline offers a principled and efficient way to reason about complex "
"systems, such as marketing data, where many drivers interact between "
"themself and over time. It replaces fragmented analyses with a unified, "
"interpretable, and uncertainty-aware causal model — brings meaning to our"
" model and the ecosystem they represent."
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740050
#: 02d4d0a0fa8b4c8fbd2faf295b947766
msgid ""
"We invite you to extend these tools—incorporate richer priors, "
"hierarchical extensions, or dynamic components—to unlock even deeper "
"causal insights and make more confident, data-driven decisions!"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740052
#: 7fd5c4569b504654be6c28a8f7a53789
msgid "References"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740053
#: 3a9a3c9e38d2430b99b35a9f6eb5f3da
msgid ""
"[PC Algorithm for Tabular Causal "
"Discovery](https://opensource.salesforce.com/causalai/latest/tutorials/PC_Algorithm_Tabular.html)"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740055
#: 5ca577daae9f4d959e58c532e5bf8156
msgid ""
"[A fast PC algorithm for high dimensional causal discovery with multi-"
"core PCs](https://arxiv.org/pdf/1502.02454)"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740057
#: 3c1f35578f5f4db89e9762233a1f99de
msgid ""
"[dcFCI: Robust Causal Discovery Under Latent Confounding, Unfaithfulness,"
" and Mixed Data](https://arxiv.org/abs/2505.06542?utm_source=chatgpt.com)"
msgstr ""

#: ../source/notebooks/mmm/mmm_causal_reasoning_and_discovery.ipynb:740059
#: 6166ee35b7a14fe084f49a34e7a03214
#, python-format
msgid ""
"[FCI Algorithm Introduction](https://causal-"
"learn.readthedocs.io/en/latest/search_methods_index/Constraint-"
"based%20causal%20discovery%20methods/FCI.html?utm_source=chatgpt.com)"
msgstr ""
