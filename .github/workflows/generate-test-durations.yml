name: Generate Test Durations

on:
  workflow_dispatch:  # Manual trigger only

permissions:
  contents: write
  pull-requests: write

jobs:
  generate-durations:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        linker: [cvm]  # Only generate once, cvm is faster
    env:
      PYTENSOR_FLAGS: linker=${{ matrix.linker }}
    steps:
      - uses: actions/checkout@v6
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.13"
          cache: "pip"
          cache-dependency-path: "pyproject.toml"
      - name: Install dependencies
        run: |
          sudo apt-get install graphviz graphviz-dev
          pip install -e .[test]
      - name: Run all tests with duration tracking
        run: |
          pytest --store-durations --durations-path .test_durations -v
        continue-on-error: true  # Generate durations even if some tests fail
      - name: Ensure newline at end of durations file
        run: |
          # Add newline if missing (pre-commit requirement)
          [ -n "$(tail -c1 .test_durations)" ] && echo >> .test_durations || true
      - name: Create Pull Request
        id: cpr
        uses: peter-evans/create-pull-request@v8
        with:
          commit-message: "chore: update test durations"
          branch: update-test-durations
          delete-branch: true
          title: "chore: update test durations"
          body: |
            ## Automated Test Durations Update

            This PR updates the `.test_durations` file with the latest test execution times from the full test suite.

            **Details:**
            - ðŸ¤– Workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            - ðŸ‘¤ Triggered by: @${{ github.actor }}
            - ðŸ“Š See workflow logs for full test statistics

            **Note:** This is an automated update. The file will be used by pytest-split to balance test groups in CI.
          labels: |
            no releasenotes
            maintenance
      - name: Enable auto-merge
        if: steps.cpr.outputs.pull-request-number != ''
        env:
          GH_TOKEN: ${{ github.token }}
        run: gh pr merge --auto --squash ${{ steps.cpr.outputs.pull-request-number }}
      - name: Display duration stats
        run: |
          echo "=== Top 20 slowest tests ==="
          python3 << 'EOF'
          import json
          with open('.test_durations') as f:
              durations = json.load(f)

          sorted_tests = sorted(durations.items(), key=lambda x: x[1], reverse=True)

          print(f"\nTotal tests: {len(durations)}")
          print(f"Total time: {sum(durations.values()):.1f}s ({sum(durations.values())/60:.1f}m)\n")

          print("Top 20 slowest tests:")
          for i, (test, dur) in enumerate(sorted_tests[:20], 1):
              print(f"{i:2}. {dur:>8.2f}s - {test}")

          # Calculate what 6-way split would look like
          total_time = sum(durations.values())
          ideal_per_group = total_time / 6
          print(f"\n=== 6-way split estimation ===")
          print(f"Ideal time per group: {ideal_per_group:.1f}s ({ideal_per_group/60:.1f}m)")
          print(f"Longest test: {sorted_tests[0][1]:.1f}s ({sorted_tests[0][1]/60:.1f}m)")
          print(f"Expected worst group: ~{max(ideal_per_group, sorted_tests[0][1])/60:.1f}m")
          EOF
